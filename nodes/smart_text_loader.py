"""
buding_SmartTextLoader - æ™ºèƒ½æ–‡æœ¬æ‰¹é‡åŠ è½½å™¨
æ”¯æŒç‰ˆæœ¬æ§åˆ¶ã€å¤šè¯­è¨€ç¼–ç ã€åå‘ç­›é€‰ã€æ—¶é—´æˆ³ç­›é€‰ã€æ–‡ä»¶å¤§å°æ§åˆ¶ã€æ™ºèƒ½æ’åºç­‰åŠŸèƒ½
"""
import os
import re
import json
import random
import time
import datetime
from pathlib import Path
from typing import List, Dict, Any, Tuple

# å°è¯•å¯¼å…¥natsortç”¨äºè‡ªç„¶æ’åº
try:
    import natsort
    NATSORT_AVAILABLE = True
except ImportError:
    NATSORT_AVAILABLE = False
    print("âš ï¸ å»ºè®®å®‰è£… natsort ä»¥è·å¾—æ›´å¥½çš„è‡ªç„¶æ’åº: pip install natsort")

# ComfyUIç›¸å…³å¯¼å…¥
try:
    from comfy.utils import ProgressBar
    # ComfyUIçš„ProgressBarä¸æ”¯æŒdescå‚æ•°ï¼Œéœ€è¦é€‚é…
    class ComfyUIProgressBar:
        def __init__(self, total):
            self.total = total
            self.pbar = ProgressBar(total)
        def update(self, value, desc=None):
            if desc:
                print(f"{desc}: {value}/{self.total}")
            self.pbar.update(value)
except ImportError:
    # å¦‚æœä¸åœ¨ComfyUIç¯å¢ƒä¸­ï¼Œæä¾›ç®€å•çš„æ›¿ä»£
    class ComfyUIProgressBar:
        def __init__(self, total):
            self.total = total
        def update(self, value, desc=None):
            if desc:
                print(f"{desc}: {value}/{self.total}")

class buding_SmartTextLoader:
    """æ™ºèƒ½æ–‡æœ¬æ‰¹é‡åŠ è½½å™¨"""
    
    def __init__(self):
        # ç¼“å­˜æœºåˆ¶ï¼šå­˜å‚¨æ‰«æç»“æœä»¥æé«˜æ€§èƒ½
        self.cache: Dict[str, Any] = {}
        
    @classmethod
    def INPUT_TYPES(cls):
        """å®šä¹‰è¾“å…¥å‚æ•°"""
        inputs = {
            "required": {
                "directory_path": ("STRING", {"default": "", "multiline": False, "tooltip": "è¦æ‰«æçš„èµ„äº§åº“æ ¹ç›®å½•è·¯å¾„"}),
                "file_extension": (
                    [".txt", ".srt", ".vtt", ".ass", ".ssa", "ä»»æ„æ–‡ä»¶"], 
                    {"default": ".txt", "tooltip": "æ–‡ä»¶æ‰©å±•åè¿‡æ»¤ï¼Œé€‰æ‹©'ä»»æ„æ–‡ä»¶'åŒ¹é…æ‰€æœ‰æ–‡æœ¬ç±»æ–‡ä»¶"}
                ),
                "scan_max_depth": ("INT", {"default": 3, "min": 0, "max": 10, "tooltip": "æ‰«æå­ç›®å½•çš„æœ€å¤§æ·±åº¦"}),
                "keywords": ("STRING", {"default": "", "multiline": True, "tooltip": "æ­£å‘åŒ¹é…å…³é”®è¯ï¼Œæ¯è¡Œä¸€ä¸ªï¼ˆæˆ–å…³ç³»ï¼‰"}),
                "similarity_threshold": ("FLOAT", {"default": 0.7, "min": 0.0, "max": 1.0, "step": 0.1, "tooltip": "æ¨¡ç³ŠåŒ¹é…çš„æœ€ä½ç›¸ä¼¼åº¦è¦æ±‚"}),
                "debug_mode": ("BOOLEAN", {"default": False, "tooltip": "å¯ç”¨è°ƒè¯•è¾“å‡ºæ¨¡å¼"}),
            },
            "optional": {
                # æ™ºèƒ½æ˜ å°„ç³»ç»Ÿ
                "enable_mapping": ("BOOLEAN", {"default": False, "tooltip": "æ˜¯å¦å¯ç”¨è¯­ä¹‰æ˜ å°„ï¼Œå°†ä»£å·æ›¿æ¢ä¸ºè§„èŒƒå…³é”®è¯"}),
                "mapping_json": ("STRING", {"default": "{\n  \"temp_01\": \"ä¸»è§’A\",\n  \"temp_02\": \"ä¸»è§’B\",\n  \"draft\": \"è‰ç¨¿ç‰ˆ\",\n  \"final\": \"æœ€ç»ˆç‰ˆ\"\n}", "multiline": True, "tooltip": "JSONæ ¼å¼çš„æ˜ å°„è¡¨ï¼Œç”¨äºè§„èŒƒåŒ–è·¯å¾„/æ–‡ä»¶å"}),
                
                # åå‘ç­›é€‰
                "enable_negative_filter": ("BOOLEAN", {"default": False, "tooltip": "å¯ç”¨åå‘åŒ¹é…æ¨¡å¼"}),
                "negative_keywords": ("STRING", {"default": "", "multiline": True, "tooltip": "åå‘æ’é™¤å…³é”®è¯ï¼Œæ¯è¡Œä¸€ä¸ª"}),
                
                # æ—¶é—´æˆ³ç­›é€‰
                "enable_time_filter": ("BOOLEAN", {"default": False, "tooltip": "å¯ç”¨æ—¶é—´æˆ³ç­›é€‰åŠŸèƒ½"}),
                "min_age_days": ("STRING", {"default": "0.0", "tooltip": "æ–‡ä»¶æœ€å°å¹´é¾„ï¼ˆå¤©ï¼‰ï¼Œ0è¡¨ç¤ºä¸é™åˆ¶"}),
                "max_age_days": ("STRING", {"default": "0.0", "tooltip": "æ–‡ä»¶æœ€å¤§å¹´é¾„ï¼ˆå¤©ï¼‰ï¼Œ0è¡¨ç¤ºä»Šå¤©"}),
                "date_filter_mode": (["ä¿®æ”¹æ—¶é—´", "åˆ›å»ºæ—¶é—´"], {"default": "ä¿®æ”¹æ—¶é—´", "tooltip": "æ—¶é—´æˆ³ç­›é€‰ç±»å‹"}),
                
                # æ–‡ä»¶å¤§å°ç­›é€‰
                "enable_size_filter": ("BOOLEAN", {"default": False, "tooltip": "å¯ç”¨æ–‡ä»¶å¤§å°ç­›é€‰åŠŸèƒ½"}),
                "min_file_size": ("INT", {"default": 0, "min": 0, "max": 10737418240, "step": 1024, "tooltip": "æœ€å°æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰ï¼Œæœ€å¤§10737418240å­—èŠ‚(10GB)"}),
                "max_file_size": ("INT", {"default": 1048576, "min": 0, "max": 10737418240, "step": 1024, "tooltip": "æœ€å¤§æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰ï¼Œæœ€å¤§10737418240å­—èŠ‚(10GB)"}),
                
                # æ’åºä¸éšæœºåŒ–
                "sort_mode": (["æ–‡ä»¶å(æ•°å­—ä¼˜å…ˆ)", "æ–‡ä»¶å(å­—æ¯)", "ä¿®æ”¹æ—¶é—´(æ–°åˆ°æ—§)", "ä¿®æ”¹æ—¶é—´(æ—§åˆ°æ–°)", "æ–‡ä»¶å¤§å°(å¤§åˆ°å°)", "æ–‡ä»¶å¤§å°(å°åˆ°å¤§)", "éšæœºæ’åº"], {"default": "æ–‡ä»¶å(æ•°å­—ä¼˜å…ˆ)", "tooltip": "æ–‡ä»¶æ’åºæ–¹å¼"}),
                "random_selection": ("BOOLEAN", {"default": False, "tooltip": "æ˜¯å¦éšæœºé€‰æ‹©æ–‡ä»¶"}),
                "seed": ("INT", {"default": 0, "min": 0, "max": 0xFFFFFFFFFFFFFFFF, "tooltip": "éšæœºç§å­ï¼Œ0è¡¨ç¤ºè‡ªåŠ¨ç”Ÿæˆ"}),
                
                # åˆ—è¡¨æ“ä½œ
                "file_limit": ("INT", {"default": 0, "min": 0, "step": 1, "tooltip": "è¾“å‡ºåˆ—è¡¨æœ€å¤§æ–‡ä»¶æ•°é‡ï¼Œ0è¡¨ç¤ºä¸é™åˆ¶"}),
                "start_index": ("INT", {"default": 0, "min": 0, "step": 1, "tooltip": "ä»åˆ—è¡¨çš„å“ªä¸ªç´¢å¼•å¼€å§‹è¾“å‡º"}),
                "select_index": ("INT", {"default": -1, "min": -1, "step": 1, "tooltip": "å¼ºåˆ¶é€‰ä¸­åˆ—è¡¨ä¸­çš„ç‰¹å®šç´¢å¼•æ–‡ä»¶ï¼Œ-1ç¦ç”¨"}),

                # æ–‡æœ¬å†…å®¹å¤„ç†
                "text_encoding": (["utf-8-sig", "utf-8", "gbk", "è‡ªåŠ¨æ£€æµ‹"], {"default": "utf-8-sig", "tooltip": "æ–‡æœ¬ç¼–ç æ ¼å¼ï¼Œä¸­æ–‡æ¨èä½¿ç”¨utf-8-sigé¿å…BOMä¹±ç "}),
                "trim_whitespace": ("BOOLEAN", {"default": True, "tooltip": "å»é™¤è¯»å–å†…å®¹çš„é¦–å°¾ç©ºç™½å­—ç¬¦"}),
                "normalize_line_endings": ("BOOLEAN", {"default": True, "tooltip": "æ ‡å‡†åŒ–æ¢è¡Œç¬¦ä¸º\\næ ¼å¼"}),
            }
        }
        return inputs

    # å®šä¹‰è¾“å‡ºç«¯å£å’Œç±»å‹
    RETURN_TYPES = ("STRING", "STRING", "STRING", "INT")
    RETURN_NAMES = ("SELECTED_CONTENT", "SELECTED_PATH", "ALL_PATHS", "FILE_COUNT")
    OUTPUT_IS_LIST = (False, False, False, False)
    FUNCTION = "load_batch"
    CATEGORY = "buding_Tools/File_Assets"
    
    @classmethod
    def IS_CHANGED(cls, directory_path, file_extension, scan_max_depth, keywords, similarity_threshold, debug_mode=False, **kwargs):
        """æ£€æŸ¥è¾“å…¥æ˜¯å¦æ”¹å˜"""
        param_string = f"{directory_path}_{file_extension}_{scan_max_depth}_{keywords}_{similarity_threshold}_{str(kwargs)}"
        return hash(param_string)

    def load_batch(self, directory_path: str, keywords: str, file_extension: str = ".txt", 
                   scan_max_depth: int = 3, similarity_threshold: float = 0.7, 
                   debug_mode: bool = False, enable_mapping: bool = False, 
                   mapping_json: str = "", enable_negative_filter: bool = False, 
                   negative_keywords: str = "", enable_time_filter: bool = False, 
                   min_age_days: str = "0.0", max_age_days: str = "0.0", 
                   date_filter_mode: str = "ä¿®æ”¹æ—¶é—´", enable_size_filter: bool = False, 
                   min_file_size: int = 0, max_file_size: int = 1048576, 
                   sort_mode: str = "æ–‡ä»¶å(æ•°å­—ä¼˜å…ˆ)", random_selection: bool = False, 
                   seed: int = 0, file_limit: int = 0, start_index: int = 0, 
                   select_index: int = -1, text_encoding: str = "utf-8-sig", 
                   trim_whitespace: bool = True, normalize_line_endings: bool = True, 
                   **kwargs: Any) -> Tuple[str, str, str, int]:
        """æ™ºèƒ½æ–‡æœ¬æ‰¹é‡åŠ è½½ä¸»å‡½æ•°"""
        
        # å‚æ•°éªŒè¯ï¼šå¤„ç†å­—ç¬¦ä¸²è½¬æ¢ä¸ºfloatå’Œint
        try:
            min_age_days = float(min_age_days) if min_age_days else 0.0
        except (ValueError, TypeError):
            min_age_days = 0.0
            
        try:
            max_age_days = float(max_age_days) if max_age_days else 0.0
        except (ValueError, TypeError):
            max_age_days = 0.0
            
        try:
            min_file_size = int(min_file_size) if min_file_size else 0
        except (ValueError, TypeError):
            min_file_size = 0
            
        try:
            max_file_size = int(max_file_size) if max_file_size else 1048576
        except (ValueError, TypeError):
            max_file_size = 1048576
        
        # åˆå§‹åŒ–è¿›åº¦æ¡
        pbar = ComfyUIProgressBar(100)
        pbar.update(5, desc="åˆå§‹åŒ–åŠ è½½å™¨...")
        
        try:
            # 1. æ‰«æä¸ç¼“å­˜
            all_files = self._scan_directory_cached(
                directory_path, scan_max_depth, file_extension, debug_mode
            )
            pbar.update(10, desc=f"æ‰¾åˆ° {len(all_files)} ä¸ªæ–‡ä»¶ã€‚")
            
            # 2. æ™ºèƒ½æ˜ å°„å¤„ç†ï¼ˆæ–°å¢æ ¸å¿ƒåŠŸèƒ½ï¼‰
            if enable_mapping:
                all_files = self._apply_semantic_mapping(all_files, mapping_json, debug_mode)
                pbar.update(15, desc="åº”ç”¨è¯­ä¹‰æ˜ å°„å®Œæˆã€‚")
            
            # 3. æ­£å‘ç­›é€‰ (æ‰©å±•å + å…³é”®è¯)
            matched_files = self._filter_positive(
                all_files, keywords, similarity_threshold, debug_mode
            )
            pbar.update(20, desc=f"æ­£å‘ç­›é€‰åå‰©ä½™ {len(matched_files)} ä¸ªæ–‡ä»¶ã€‚")
            
            # 4. åå‘ç­›é€‰
            if enable_negative_filter:
                matched_files = self._filter_negative(
                    matched_files, negative_keywords, debug_mode
                )
                pbar.update(30, desc=f"åå‘ç­›é€‰åå‰©ä½™ {len(matched_files)} ä¸ªæ–‡ä»¶ã€‚")
            
            # 5. æ—¶é—´æˆ³ç­›é€‰
            if enable_time_filter:
                matched_files = self._filter_by_timestamp(
                    matched_files, min_age_days, max_age_days, 
                    date_filter_mode, debug_mode
                )
                pbar.update(40, desc=f"æ—¶é—´ç­›é€‰åå‰©ä½™ {len(matched_files)} ä¸ªæ–‡ä»¶ã€‚")
                
            # 6. æ–‡ä»¶å¤§å°ç­›é€‰
            if enable_size_filter:
                matched_files = self._filter_by_file_size(
                    matched_files, min_file_size, max_file_size, debug_mode
                )
                pbar.update(50, desc=f"å¤§å°ç­›é€‰åå‰©ä½™ {len(matched_files)} ä¸ªæ–‡ä»¶ã€‚")
            
            # 7. æ™ºèƒ½æ’åº
            matched_files = self._apply_smart_sorting(matched_files, sort_mode)
            pbar.update(60, desc="å®Œæˆæ’åºã€‚")
            
            # 8. åº”ç”¨ç´¢å¼•å’Œé™åˆ¶
            final_files = self._apply_limits_and_selection(matched_files, {
                'random_selection': random_selection,
                'seed': seed,
                'file_limit': file_limit,
                'start_index': start_index,
                'select_index': select_index,
                'debug_mode': debug_mode
            })
            pbar.update(70, desc=f"æœ€ç»ˆè¾“å‡º {len(final_files)} ä¸ªæ–‡ä»¶ã€‚")
            
            # 9. å‡†å¤‡è¾“å‡ºæ•°æ®
            # æ³¨æ„ï¼šä½¿ç”¨åŸå§‹è·¯å¾„è¿›è¡Œè¾“å‡ºï¼Œæ˜ å°„ä»…ç”¨äºåŒ¹é…
            all_paths_list = []
            for f in final_files:
                # å¦‚æœæœ‰åŸå§‹è·¯å¾„ï¼ˆç»è¿‡æ˜ å°„ï¼‰ï¼Œä½¿ç”¨åŸå§‹è·¯å¾„ï¼›å¦åˆ™ä½¿ç”¨å½“å‰è·¯å¾„
                original_path = f.get('original_path', f['path'])
                all_paths_list.append(original_path)
            
            all_paths_json = json.dumps(all_paths_list, ensure_ascii=False)
            file_count = len(all_paths_list)
            
            selected_content, selected_path = "", ""

            if all_paths_list:
                selected_path = all_paths_list[0]
                # 10. è¯»å–ç¬¬ä¸€ä¸ªæ–‡ä»¶å†…å®¹
                selected_content = self._load_file_content(
                    selected_path, text_encoding, 
                    trim_whitespace, normalize_line_endings,
                    debug_mode
                )
                pbar.update(90, desc="åŠ è½½å†…å®¹å®Œæˆã€‚")

            pbar.update(100, desc="å¤„ç†å®Œæ¯•ã€‚")
            
            if debug_mode:
                print(f"ğŸ‰ æ™ºèƒ½æ–‡æœ¬åŠ è½½å®Œæˆ: {file_count} ä¸ªæ–‡ä»¶")
                if selected_path:
                    print(f"ğŸ“„ é€‰ä¸­æ–‡ä»¶: {os.path.basename(selected_path)}")

            return (selected_content, selected_path, all_paths_json, file_count)
            
        except Exception as e:
            error_msg = f"âŒ æ™ºèƒ½æ–‡æœ¬åŠ è½½å¤±è´¥: {str(e)}"
            if debug_mode:
                print(error_msg)
                import traceback
                traceback.print_exc()
            return ("", "", "[]", 0)

    def _scan_directory_cached(self, root_dir: str, max_depth: int, extension: str, debug_mode: bool) -> List[Dict]:
        """æ‰«æç›®å½•å¹¶ä½¿ç”¨ç¼“å­˜"""
        # æ¸…ç†è·¯å¾„
        root_dir = root_dir.strip().strip('"\'')
        
        if not root_dir or not os.path.exists(root_dir):
            if debug_mode:
                print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {root_dir}")
            return []
        
        # ä½¿ç”¨æ ¹ç›®å½•ã€æ·±åº¦ã€æ‰©å±•åå’Œç›®å½•ä¿®æ”¹æ—¶é—´ä½œä¸ºç¼“å­˜é”®
        try:
            dir_mtime = os.path.getmtime(root_dir)
        except OSError:
            dir_mtime = 0
            
        cache_key = f"{root_dir}|{max_depth}|{extension}|{dir_mtime}"
        
        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
        if cache_key in self.cache:
            if debug_mode:
                print(f"ğŸ“š ä½¿ç”¨ç¼“å­˜ï¼š{root_dir}")
            return self.cache[cache_key]
        
        # æ‰§è¡Œæ·±åº¦é€’å½’æ‰«æ
        if debug_mode:
            print(f"ğŸ”„ æ­£åœ¨æ‰«æç›®å½•ï¼š{root_dir} (æ·±åº¦: {max_depth}, æ‰©å±•å: {extension})")
        
        all_files = []
        root_path = Path(root_dir)
        
        try:
            for root, dirs, files in os.walk(root_dir):
                # è®¡ç®—å½“å‰æ·±åº¦
                current_depth = len(Path(root).relative_to(root_path).parts)
                if current_depth > max_depth:
                    dirs[:] = []  # é˜»æ­¢ os.walk ç»§ç»­å‘ä¸‹æœç´¢
                    continue
                    
                for file in files:
                    # æ‰©å±•åè¿‡æ»¤
                    if extension != "ä»»æ„æ–‡ä»¶" and not file.lower().endswith(extension.lower()):
                        continue
                    
                    file_path = os.path.join(root, file)
                    
                    try:
                        # è®°å½•æ–‡ä»¶ä¿¡æ¯
                        file_info = {
                            'path': file_path,
                            'filename': file,
                            'clean_name': self._clean_filename_for_match(file),
                            'mtime': os.path.getmtime(file_path),
                            'ctime': os.path.getctime(file_path),
                            'size': os.path.getsize(file_path),
                        }
                        all_files.append(file_info)
                    except OSError:
                        # è·³è¿‡æ— æ³•è®¿é—®çš„æ–‡ä»¶
                        continue
                        
        except Exception as e:
            if debug_mode:
                print(f"âŒ ç›®å½•æ‰«æå¤±è´¥: {e}")
            return []
        
        # å­˜å‚¨åˆ°ç¼“å­˜
        self.cache[cache_key] = all_files
        
        if debug_mode:
            print(f"âœ… æ‰«æå®Œæˆ: æ‰¾åˆ° {len(all_files)} ä¸ªæ–‡ä»¶")
        
        return all_files

    def _apply_semantic_mapping(self, files: List[Dict], mapping_json: str, debug_mode: bool) -> List[Dict]:
        """åº”ç”¨è¯­ä¹‰æ˜ å°„ï¼Œå°†æ–‡ä»¶è·¯å¾„ä¸­çš„ä»£å·æ›¿æ¢ä¸ºè§„èŒƒåŒ–å…³é”®è¯"""
        if not mapping_json or not mapping_json.strip():
            if debug_mode:
                print("âš ï¸ æ˜ å°„JSONä¸ºç©ºï¼Œè·³è¿‡è¯­ä¹‰æ˜ å°„")
            return files
        
        try:
            # è§£ææ˜ å°„JSON
            mapping_dict = json.loads(mapping_json)
            if not isinstance(mapping_dict, dict):
                if debug_mode:
                    print("âŒ æ˜ å°„JSONæ ¼å¼é”™è¯¯ï¼šå¿…é¡»æ˜¯å­—å…¸æ ¼å¼")
                return files
            
            if debug_mode:
                print(f"ğŸ—ºï¸ åº”ç”¨è¯­ä¹‰æ˜ å°„ï¼Œå…± {len(mapping_dict)} æ¡è§„åˆ™")
                for old, new in mapping_dict.items():
                    print(f"  â€¢ {old} â†’ {new}")
            
            # å¯¹æ¯ä¸ªæ–‡ä»¶åº”ç”¨æ˜ å°„
            mapped_files = []
            for file_info in files:
                original_path = file_info['path']
                original_filename = file_info['filename']
                original_clean_name = file_info['clean_name']
                
                # åº”ç”¨æ˜ å°„åˆ°å®Œæ•´è·¯å¾„
                mapped_path = original_path
                mapped_filename = original_filename
                mapped_clean_name = original_clean_name
                
                # å¯¹è·¯å¾„ä¸­çš„æ¯ä¸ªéƒ¨åˆ†åº”ç”¨æ˜ å°„
                path_parts = original_path.replace('\\', '/').split('/')
                mapped_parts = []
                
                for part in path_parts:
                    mapped_part = part
                    for old_term, new_term in mapping_dict.items():
                        if old_term in mapped_part:
                            mapped_part = mapped_part.replace(old_term, new_term)
                    mapped_parts.append(mapped_part)
                
                mapped_path = '\\'.join(mapped_parts) if '\\' in original_path else '/'.join(mapped_parts)
                
                # å¯¹æ–‡ä»¶ååº”ç”¨æ˜ å°„
                for old_term, new_term in mapping_dict.items():
                    if old_term in mapped_filename:
                        mapped_filename = mapped_filename.replace(old_term, new_term)
                
                # é‡æ–°è®¡ç®—æ˜ å°„åçš„clean_name
                mapped_clean_name = self._clean_filename_for_match(mapped_filename)
                
                # åˆ›å»ºæ–°çš„æ–‡ä»¶ä¿¡æ¯å¯¹è±¡
                mapped_file_info = file_info.copy()
                mapped_file_info.update({
                    'path': mapped_path,
                    'filename': mapped_filename,
                    'clean_name': mapped_clean_name,
                    'original_path': original_path,  # ä¿ç•™åŸå§‹è·¯å¾„ç”¨äºæœ€ç»ˆè¾“å‡º
                })
                
                mapped_files.append(mapped_file_info)
                
                if debug_mode and (mapped_path != original_path or mapped_filename != original_filename):
                    print(f"  ğŸ”„ {original_path} â†’ {mapped_path}")
            
            if debug_mode:
                print(f"âœ… è¯­ä¹‰æ˜ å°„å®Œæˆï¼Œå¤„ç†äº† {len(mapped_files)} ä¸ªæ–‡ä»¶")
            
            return mapped_files
            
        except json.JSONDecodeError as e:
            if debug_mode:
                print(f"âŒ æ˜ å°„JSONè§£æå¤±è´¥: {e}")
            return files
        except Exception as e:
            if debug_mode:
                print(f"âŒ è¯­ä¹‰æ˜ å°„åº”ç”¨å¤±è´¥: {e}")
            return files

    def _clean_filename_for_match(self, filename: str) -> str:
        """æ¸…ç†æ–‡ä»¶åï¼Œç”¨äºæ¨¡ç³ŠåŒ¹é…"""
        # ç§»é™¤æ–‡ä»¶æ‰©å±•å
        name = os.path.splitext(filename)[0]
        
        # ç§»é™¤å¸¸è§çš„ç‰ˆæœ¬å·å’Œåˆ†éš”ç¬¦
        name = re.sub(r'[_\-\s]+', ' ', name)
        
        # ç§»é™¤æ•°å­—ç‰ˆæœ¬æ ‡è¯† (å¦‚ v1, v2, _v1, _v2)
        name = re.sub(r'[ _]?[vV][0-9]+', '', name)
        
        # ç§»é™¤çº¯æ•°å­—ï¼ˆä½†ä¿ç•™ä¸­æ–‡æ•°å­—ï¼‰
        name = re.sub(r'\b\d+\b', '', name)
        
        # åªä¿ç•™å­—æ¯ã€ä¸­æ–‡ã€ç©ºæ ¼ã€åŸºæœ¬æ ‡ç‚¹
        name = re.sub(r'[^\w\u4e00-\u9fff\s\-_\.\(\)\[\]]', '', name)
        
        return name.lower().strip()

    def _calculate_similarity(self, clean_keyword: str, file_info: Dict) -> float:
        """ä½¿ç”¨ difflib è®¡ç®—æ¨¡ç³Šç›¸ä¼¼åº¦"""
        import difflib
        clean_filename = file_info['clean_name']
        
        if not clean_keyword or not clean_filename:
            return 0.0

        # SequenceMatcherè®¡ç®—ç¼–è¾‘è·ç¦»ç›¸ä¼¼åº¦
        similarity = difflib.SequenceMatcher(None, clean_keyword, clean_filename).ratio()
        
        # ç²¾ç¡®åŒ¹é…åŠ åˆ†
        if clean_keyword == clean_filename:
            similarity = min(1.0, similarity + 0.2)
        # åŒ…å«åŒ¹é…åŠ åˆ†
        elif clean_keyword in clean_filename or clean_filename in clean_keyword:
            similarity = min(1.0, similarity + 0.1)
            
        return similarity

    def _filter_positive(self, files: List[Dict], keywords_str: str, threshold: float, debug_mode: bool) -> List[Dict]:
        """æ­£å‘ç­›é€‰ï¼šæ–‡ä»¶åå¿…é¡»åŒ…å«ä»»ä¸€å…³é”®è¯ï¼Œä¸”ç›¸ä¼¼åº¦è¾¾æ ‡"""
        if not keywords_str:
            return files

        keywords = [kw.strip().lower() for kw in keywords_str.split('\n') if kw.strip()]
        
        if debug_mode:
            print(f"ğŸ” æ­£å‘ç­›é€‰å…³é”®è¯: {keywords}")
        
        filtered_files = []
        for file_info in files:
            for keyword in keywords:
                # ç®€å•åŒ…å«åŒ¹é…
                if keyword in file_info['filename'].lower():
                    # æ¨¡ç³ŠåŒ¹é…éªŒè¯
                    similarity = self._calculate_similarity(keyword, file_info)
                    
                    if similarity >= threshold:
                        file_info['match_score'] = similarity
                        file_info['match_keyword'] = keyword
                        filtered_files.append(file_info)
                        if debug_mode:
                            print(f"  âœ… {file_info['filename']} åŒ¹é…å…³é”®è¯ '{keyword}' (ç›¸ä¼¼åº¦: {similarity:.3f})")
                        break
                else:
                    # å³ä½¿ç®€å•åŒ…å«ä¸åŒ¹é…ï¼Œä¹Ÿå°è¯•æ¨¡ç³ŠåŒ¹é…
                    clean_keyword = self._clean_filename_for_match(keyword)
                    similarity = self._calculate_similarity(clean_keyword, file_info)
                    
                    if similarity >= threshold:
                        file_info['match_score'] = similarity
                        file_info['match_keyword'] = keyword
                        filtered_files.append(file_info)
                        if debug_mode:
                            print(f"  ğŸ¯ {file_info['filename']} æ¨¡ç³ŠåŒ¹é…å…³é”®è¯ '{keyword}' (ç›¸ä¼¼åº¦: {similarity:.3f})")
                        break
                        
        if debug_mode:
            print(f"âœ… æ­£å‘ç­›é€‰ç»“æœ: {len(filtered_files)}/{len(files)} ä¸ªæ–‡ä»¶")
        
        return filtered_files

    def _filter_negative(self, files: List[Dict], negative_keywords_str: str, debug_mode: bool) -> List[Dict]:
        """åå‘ç­›é€‰ï¼šç§»é™¤æ–‡ä»¶ååŒ…å«æŒ‡å®šå…³é”®è¯çš„æ–‡ä»¶"""
        if not negative_keywords_str:
            return files
        
        negative_keywords = [kw.strip().lower() for kw in negative_keywords_str.split('\n') if kw.strip()]
        
        if debug_mode:
            print(f"ğŸš« åå‘ç­›é€‰å…³é”®è¯: {negative_keywords}")
        
        filtered_files = []
        excluded_count = 0
        
        for file_info in files:
            # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ…å«ä»»ä½•ä¸€ä¸ªåå‘å…³é”®è¯
            is_negative_match = any(kw in file_info['filename'].lower() for kw in negative_keywords)
            
            if not is_negative_match:
                filtered_files.append(file_info)
            else:
                excluded_count += 1
                if debug_mode:
                    matched_keywords = [kw for kw in negative_keywords if kw in file_info['filename'].lower()]
                    print(f"  ğŸš« æ’é™¤ {file_info['filename']} (åŒ¹é…: {matched_keywords})")
        
        if debug_mode:
            print(f"âœ… åå‘ç­›é€‰ç»“æœ: æ’é™¤ {excluded_count} ä¸ªæ–‡ä»¶ï¼Œå‰©ä½™ {len(filtered_files)} ä¸ª")
        
        return filtered_files

    def _filter_by_timestamp(self, files: List[Dict], min_age_days: float, max_age_days: float, 
                           date_filter_mode: str, debug_mode: bool) -> List[Dict]:
        """æŒ‰æ—¶é—´æˆ³ç­›é€‰æ–‡ä»¶"""
        if min_age_days == 0.0 and max_age_days == 0.0:
            return files
        
        now = datetime.datetime.now()
        min_time = now - datetime.timedelta(days=min_age_days) if min_age_days > 0 else None
        max_time = now - datetime.timedelta(days=max_age_days) if max_age_days > 0 else now
        
        if debug_mode:
            print(f"â° æ—¶é—´ç­›é€‰: {min_age_days}-{max_age_days} å¤©å‰ ({date_filter_mode})")
            if min_time:
                print(f"   æœ€æ—©æ—¶é—´: {min_time.strftime('%Y-%m-%d %H:%M:%S')}")
            if max_time and max_age_days > 0:
                print(f"   æœ€æ™šæ—¶é—´: {max_time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        filtered_files = []
        excluded_count = 0
        
        for file_info in files:
            file_path = file_info['path']
            
            try:
                # è·å–æ–‡ä»¶æ—¶é—´æˆ³
                if date_filter_mode == "ä¿®æ”¹æ—¶é—´":
                    timestamp = file_info['mtime']
                else:  # "åˆ›å»ºæ—¶é—´"
                    timestamp = file_info['ctime']
                
                file_time = datetime.datetime.fromtimestamp(timestamp)
                
                # æ£€æŸ¥æ—¶é—´èŒƒå›´
                if min_time and file_time < min_time:
                    excluded_count += 1
                    if debug_mode:
                        print(f"  â° æ’é™¤ {file_info['filename']} (å¤ªæ—§: {file_time.strftime('%Y-%m-%d')})")
                    continue
                if max_time and max_age_days > 0 and file_time > max_time:
                    excluded_count += 1
                    if debug_mode:
                        print(f"  â° æ’é™¤ {file_info['filename']} (å¤ªæ–°: {file_time.strftime('%Y-%m-%d')})")
                    continue
                
                filtered_files.append(file_info)
                
            except OSError:
                continue
        
        if debug_mode:
            print(f"âœ… æ—¶é—´ç­›é€‰ç»“æœ: æ’é™¤ {excluded_count} ä¸ªæ–‡ä»¶ï¼Œå‰©ä½™ {len(filtered_files)} ä¸ª")
        
        return filtered_files

    def _filter_by_file_size(self, files: List[Dict], min_size: int, max_size: int, debug_mode: bool) -> List[Dict]:
        """æŒ‰æ–‡ä»¶å¤§å°ç­›é€‰"""
        if debug_mode:
            print(f"ğŸ“ å¤§å°ç­›é€‰: {min_size}-{max_size} å­—èŠ‚")
        
        filtered_files = []
        excluded_count = 0
        
        for file_info in files:
            file_size = file_info['size']
            
            # æ£€æŸ¥å¤§å°èŒƒå›´
            if min_size > 0 and file_size < min_size:
                excluded_count += 1
                if debug_mode:
                    print(f"  ğŸ“ æ’é™¤ {file_info['filename']} (å¤ªå°: {file_size} å­—èŠ‚)")
                continue
            if max_size > 0 and file_size > max_size:
                excluded_count += 1
                if debug_mode:
                    size_kb = file_size / 1024
                    print(f"  ğŸ“ æ’é™¤ {file_info['filename']} (å¤ªå¤§: {size_kb:.1f} KB)")
                continue
            
            filtered_files.append(file_info)
        
        if debug_mode:
            print(f"âœ… å¤§å°ç­›é€‰ç»“æœ: æ’é™¤ {excluded_count} ä¸ªæ–‡ä»¶ï¼Œå‰©ä½™ {len(filtered_files)} ä¸ª")
        
        return filtered_files

    def _apply_smart_sorting(self, files: List[Dict], sort_mode: str) -> List[Dict]:
        """æ™ºèƒ½æ’åºç®—æ³•"""
        if not files:
            return files
        
        if sort_mode == "æ–‡ä»¶å(æ•°å­—ä¼˜å…ˆ)":
            if NATSORT_AVAILABLE:
                # ä½¿ç”¨natsortåº“è¿›è¡Œè‡ªç„¶æ’åº
                return natsort.natsorted(files, key=lambda x: x['filename'])
            else:
                # å›é€€åˆ°è‡ªå®šä¹‰è‡ªç„¶æ’åº
                def natural_key(filename):
                    return [int(text) if text.isdigit() else text.lower() 
                           for text in re.split(r'(\d+)', filename)]
                return sorted(files, key=lambda x: natural_key(x['filename']))
        
        elif sort_mode == "æ–‡ä»¶å(å­—æ¯)":
            return sorted(files, key=lambda x: x['filename'].lower())
        
        elif sort_mode == "ä¿®æ”¹æ—¶é—´(æ–°åˆ°æ—§)":
            return sorted(files, key=lambda x: x['mtime'], reverse=True)
        
        elif sort_mode == "ä¿®æ”¹æ—¶é—´(æ—§åˆ°æ–°)":
            return sorted(files, key=lambda x: x['mtime'])
        
        elif sort_mode == "æ–‡ä»¶å¤§å°(å¤§åˆ°å°)":
            return sorted(files, key=lambda x: x['size'], reverse=True)
        
        elif sort_mode == "æ–‡ä»¶å¤§å°(å°åˆ°å¤§)":
            return sorted(files, key=lambda x: x['size'])
        
        elif sort_mode == "éšæœºæ’åº":
            # æ³¨æ„ï¼šè¿™é‡Œä¸ä½¿ç”¨ç§å­ï¼Œç§å­åœ¨åé¢çš„éšæœºé€‰æ‹©ä¸­ä½¿ç”¨
            import random
            shuffled = files.copy()
            random.shuffle(shuffled)
            return shuffled
        
        else:
            return files

    def _apply_limits_and_selection(self, files: List[Dict], kwargs: Dict) -> List[Dict]:
        """åº”ç”¨ç´¢å¼•é™åˆ¶å’Œé€‰æ‹©"""
        if not files:
            return []
        
        result_files = files.copy()
        
        # éšæœºé€‰æ‹©
        if kwargs['random_selection']:
            used_seed = kwargs['seed']
            if used_seed == 0:
                used_seed = random.randint(1, 2_147_483_647)
            
            rng = random.Random(used_seed)
            rng.shuffle(result_files)
            
            if kwargs['debug_mode']:
                print(f"ğŸ² éšæœºé€‰æ‹©ä½¿ç”¨ç§å­: {used_seed}")
        
        # åº”ç”¨èµ·å§‹ç´¢å¼•
        if kwargs['start_index'] > 0:
            if kwargs['start_index'] < len(result_files):
                result_files = result_files[kwargs['start_index']:]
            else:
                result_files = []
        
        # åº”ç”¨æ•°é‡é™åˆ¶
        if kwargs['file_limit'] > 0 and len(result_files) > kwargs['file_limit']:
            result_files = result_files[:kwargs['file_limit']]
        
        # å¼ºåˆ¶é€‰æ‹©ç‰¹å®šç´¢å¼•
        if kwargs['select_index'] >= 0 and kwargs['select_index'] < len(result_files):
            selected_file = result_files[kwargs['select_index']]
            result_files = [selected_file]
        
        return result_files

    def _load_file_content(self, file_path: str, encoding: str, trim_whitespace: bool, 
                          normalize_line_endings: bool, debug: bool) -> str:
        """æ ¹æ®ç¼–ç åŠ è½½æ–‡ä»¶å†…å®¹"""
        content = ""
        
        try:
            # 1. ç¼–ç æ£€æµ‹å’ŒåŠ è½½
            if encoding == "è‡ªåŠ¨æ£€æµ‹":
                encodings_to_try = ['utf-8-sig', 'utf-8', 'gbk', 'utf-16', 'latin-1']
                used_encoding = None
                
                for enc in encodings_to_try:
                    try:
                        with open(file_path, 'r', encoding=enc) as f:
                            content = f.read()
                            used_encoding = enc
                            break
                    except UnicodeDecodeError:
                        continue
                
                if used_encoding is None:
                    # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œä½¿ç”¨é”™è¯¯å¤„ç†æ¨¡å¼
                    with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
                        content = f.read()
                    used_encoding = 'utf-8-replace'
                    
                if debug_mode:
                    print(f"ğŸ“ è‡ªåŠ¨æ£€æµ‹ç¼–ç : {used_encoding}")
            else:
                # ä½¿ç”¨æŒ‡å®šç¼–ç 
                with open(file_path, 'r', encoding=encoding) as f:
                    content = f.read()
                if debug_mode:
                    print(f"ğŸ“ ä½¿ç”¨æŒ‡å®šç¼–ç : {encoding}")

            # 2. å†…å®¹åå¤„ç†
            if normalize_line_endings:
                # æ ‡å‡†åŒ–æ¢è¡Œç¬¦ (Windows/Linux/Mac ç»Ÿä¸€ä¸º \n)
                content = content.replace('\r\n', '\n').replace('\r', '\n')
            
            if trim_whitespace:
                content = content.strip()
                
            return content
            
        except Exception as e:
            error_msg = f"[READ ERROR] æ–‡ä»¶: {os.path.basename(file_path)}, ç¼–ç : {encoding}, é”™è¯¯: {str(e)}"
            if debug_mode:
                print(error_msg)
            return error_msg


# æ³¨å†ŒèŠ‚ç‚¹
NODE_CLASS_MAPPINGS = {
    "buding_SmartTextLoader": buding_SmartTextLoader,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "buding_SmartTextLoader": "ğŸ“ æ™ºèƒ½æ–‡æœ¬æ‰¹é‡åŠ è½½å™¨",
}
