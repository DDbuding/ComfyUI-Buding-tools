import numpy as np
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

class buding_ConditionalAudio:
    """
    æ¡ä»¶éŸ³é¢‘å¼€å…³ï¼šæ ¹æ®å¸ƒå°”å€¼æ§åˆ¶éŸ³é¢‘æ•°æ®çš„è¾“å‡º
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "audio_input": ("AUDIO", {
                    "tooltip": "æ¥è‡ªä¸Šæ¸¸åŠ è½½èŠ‚ç‚¹çš„åŸå§‹éŸ³é¢‘æ•°æ®"
                }),
                "enable": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "æ¥è‡ª buding_RoleMatcher çš„æ§åˆ¶ä¿¡å·ï¼ŒTrue=è¾“å‡ºéŸ³é¢‘ï¼ŒFalse=é˜»æ–­éŸ³é¢‘"
                }),
                "debug_mode": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "å¼€å¯åä¼šåœ¨æ§åˆ¶å°æ˜¾ç¤ºéŸ³é¢‘å¼€å…³çŠ¶æ€"
                }),
                "volume_normalization": ("FLOAT", {
                    "default": -20.0,
                    "min": -60.0,
                    "max": 0.0,
                    "step": 1.0,
                    "tooltip": "éŸ³é‡æ ‡å‡†åŒ–æ°´å¹³(dB)ï¼Œ-20dBé€‚åˆå¤§å¤šæ•°åœºæ™¯ï¼Œè´Ÿå€¼å‡å°éŸ³é‡ï¼Œæ­£å€¼å¢å¤§éŸ³é‡ï¼Œè®¾ä¸º-60å¯ç¦ç”¨éŸ³é‡æ ‡å‡†åŒ–"
                }),
            }
        }
    
    RETURN_TYPES = ("AUDIO",)
    RETURN_NAMES = ("audio_output",)
    FUNCTION = "conditional_output"
    CATEGORY = "buding_Tools/Audio/Control"
    
    def conditional_output(self, audio_input, enable, debug_mode, volume_normalization=-20.0):
        """
        æ ¹æ®æ§åˆ¶ä¿¡å·æ¡ä»¶è¾“å‡ºéŸ³é¢‘
        
        å‚æ•°:
            audio_input: è¾“å…¥çš„éŸ³é¢‘æ•°æ®
            enable: æ§åˆ¶ä¿¡å·
            debug_mode: æ˜¯å¦å¼€å¯è°ƒè¯•æ¨¡å¼
            
        è¿”å›:
            tuple: åŒ…å«éŸ³é¢‘æ•°æ®æˆ–Noneçš„å…ƒç»„
        """
        try:
            if debug_mode:
                self._debug_output(enable, audio_input is not None)
            
            if enable:
                # å¯ç”¨çŠ¶æ€ï¼šå¤„ç†éŸ³é¢‘æ•°æ®
                processed_audio = audio_input
                
                # éŸ³é‡æ ‡å‡†åŒ–ï¼ˆ-60dBè¡¨ç¤ºç¦ç”¨ï¼‰
                if audio_input and volume_normalization > -60:
                    processed_audio = self._normalize_volume(audio_input, volume_normalization)
                
                # å†…ç½®æ·¡å…¥æ•ˆæœï¼ˆä½¿ç”¨å›ºå®š2200æ ·æœ¬æ•°ï¼Œçº¦50ms@44.1kHzï¼‰
                if audio_input:
                    processed_audio = self._apply_gentle_fade_in(processed_audio, 2200)
                
                return (processed_audio,)
            else:
                # ç¦ç”¨çŠ¶æ€ï¼šè¿”å›Noneæ¥é˜»æ–­æ•°æ®æµ
                # ComfyUIä¼šè·³è¿‡æ¥æ”¶Noneçš„ä¸‹æ¸¸èŠ‚ç‚¹
                return (None,)
                
        except Exception as e:
            print(f"âŒ æ¡ä»¶éŸ³é¢‘å¼€å…³é”™è¯¯: {e}")
            # å‡ºé”™æ—¶è¿”å›Noneä»¥ç¡®ä¿å®‰å…¨
            return (None,)
    
    def _debug_output(self, enable, has_audio):
        """
        è°ƒè¯•ä¿¡æ¯è¾“å‡º
        
        å‚æ•°:
            enable: æ§åˆ¶ä¿¡å·çŠ¶æ€
            has_audio: æ˜¯å¦æœ‰éŸ³é¢‘è¾“å…¥
        """
        print("\nğŸµ === æ¡ä»¶éŸ³é¢‘è°ƒè¯•ä¿¡æ¯ ===")
        print(f"ğŸ”› æ§åˆ¶ä¿¡å·: {'å¯ç”¨' if enable else 'ç¦ç”¨'}")
        print(f"ğŸ§ éŸ³é¢‘è¾“å…¥: {'æœ‰æ•ˆ' if has_audio else 'æ— æ•ˆ/None'}")
        
        if enable:
            if has_audio:
                print("âœ… éŸ³é¢‘å°†é€šè¿‡å¼€å…³ä¼ é€’åˆ°ä¸‹æ¸¸èŠ‚ç‚¹")
            else:
                print("âš ï¸ å¼€å…³å·²å¯ç”¨ï¼Œä½†éŸ³é¢‘è¾“å…¥ä¸ºæ— æ•ˆæ•°æ®")
        else:
            print("ğŸš« éŸ³é¢‘è¢«é˜»æ–­ï¼Œä¸‹æ¸¸èŠ‚ç‚¹å°†ä¸ä¼šæ‰§è¡Œ")
        
        print("ğŸµ === è°ƒè¯•ä¿¡æ¯ç»“æŸ ===\n")
    
    def _normalize_volume(self, audio_input, target_db=-20.0):
        """
        æ ‡å‡†åŒ–éŸ³é¢‘éŸ³é‡åˆ°ç›®æ ‡dBæ°´å¹³
        
        å‚æ•°:
            audio_input: è¾“å…¥éŸ³é¢‘æ•°æ®
            target_db: ç›®æ ‡éŸ³é‡æ°´å¹³(dB)
            
        è¿”å›:
            å¤„ç†åçš„éŸ³é¢‘æ•°æ®
        """
        if not audio_input:
            return audio_input
            
        try:
            waveform = audio_input['waveform']
            
            # è®¡ç®—å½“å‰RMSå€¼
            if TORCH_AVAILABLE and isinstance(waveform, torch.Tensor):
                rms = torch.sqrt(torch.mean(waveform ** 2))
                
                # é¿å…é™¤é›¶é”™è¯¯
                if rms < 1e-8:
                    return audio_input
                
                # è®¡ç®—éœ€è¦çš„å¢ç›Šä»¥è¾¾åˆ°ç›®æ ‡dBæ°´å¹³
                current_db = 20 * torch.log10(rms)
                gain_db = target_db - current_db.item()
                gain_linear = 10 ** (gain_db / 20)
                
                # åº”ç”¨å¢ç›Šï¼Œæ‰©å¤§å¯è°ƒèŠ‚èŒƒå›´
                max_gain = 100.0  # æœ€å¤§100å€å¢ç›Šï¼Œæ”¯æŒæ›´å¤§çš„éŸ³é‡è°ƒèŠ‚èŒƒå›´
                gain_linear = min(gain_linear, max_gain)
                
                # åº”ç”¨å¢ç›Š
                normalized_waveform = waveform * gain_linear
                
                # ç®€å•çš„é™å¹…å™¨é˜²æ­¢å‰Šæ³¢
                max_val = torch.max(torch.abs(normalized_waveform))
                if max_val > 0.95:
                    normalized_waveform = normalized_waveform * (0.95 / max_val)
                
                # è¿”å›å¤„ç†åçš„éŸ³é¢‘
                return {
                    'waveform': normalized_waveform, 
                    'sample_rate': audio_input['sample_rate']
                }
            else:
                # ä½¿ç”¨numpyå¤„ç†
                waveform_np = np.array(waveform)
                rms = np.sqrt(np.mean(waveform_np ** 2))
                
                if rms < 1e-8:
                    return audio_input
                    
                current_db = 20 * np.log10(rms)
                gain_db = target_db - current_db
                gain_linear = 10 ** (gain_db / 20)
                gain_linear = min(gain_linear, 100.0)  # æ‰©å¤§å¯è°ƒèŠ‚èŒƒå›´ï¼Œæœ€å¤§100å€å¢ç›Š
                
                normalized_waveform = waveform_np * gain_linear
                
                # é™å¹…å™¨
                max_val = np.max(np.abs(normalized_waveform))
                if max_val > 0.95:
                    normalized_waveform = normalized_waveform * (0.95 / max_val)
                    
                return {
                    'waveform': normalized_waveform, 
                    'sample_rate': audio_input['sample_rate']
                }
                
        except Exception as e:
            print(f"âŒ éŸ³é‡æ ‡å‡†åŒ–å¤±è´¥: {e}")
            return audio_input
    
    def _apply_gentle_fade_in(self, audio_input, transition_samples=2200):
        """
        åº”ç”¨æçŸ­çš„æ·¡å…¥æ•ˆæœï¼Œé¿å…å½±å“éŸ³è‰²
        
        å‚æ•°:
            audio_input: è¾“å…¥éŸ³é¢‘æ•°æ®
            transition_samples: è¿‡æ¸¡æ ·æœ¬æ•°
            
        è¿”å›:
            å¤„ç†åçš„éŸ³é¢‘æ•°æ®
        """
        if not audio_input:
            return audio_input
            
        try:
            waveform = audio_input['waveform']
            
            # æ£€æŸ¥éŸ³é¢‘é•¿åº¦
            if waveform.shape[-1] <= transition_samples:
                return audio_input  # éŸ³é¢‘å¤ªçŸ­ï¼Œä¸éœ€è¦æ·¡å…¥
                
            # åˆ›å»ºæ·¡å…¥æ›²çº¿
            if TORCH_AVAILABLE and isinstance(waveform, torch.Tensor):
                # ä½¿ç”¨cosineæ›²çº¿ï¼Œæ›´å¹³æ»‘
                fade_curve = 0.5 * (1 - torch.cos(torch.linspace(0, torch.pi, transition_samples)))
                
                # å…‹éš†æ³¢å½¢é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
                faded_waveform = waveform.clone()
                
                # åº”ç”¨æ·¡å…¥æ•ˆæœ
                faded_waveform[..., :transition_samples] *= fade_curve
                
                return {
                    'waveform': faded_waveform, 
                    'sample_rate': audio_input['sample_rate']
                }
            else:
                # ä½¿ç”¨numpyå¤„ç†
                fade_curve = 0.5 * (1 - np.cos(np.linspace(0, np.pi, transition_samples)))
                
                # å¤åˆ¶æ³¢å½¢é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
                faded_waveform = np.array(waveform, copy=True)
                
                # åº”ç”¨æ·¡å…¥æ•ˆæœ
                faded_waveform[..., :transition_samples] *= fade_curve
                
                return {
                    'waveform': faded_waveform, 
                    'sample_rate': audio_input['sample_rate']
                }
                
        except Exception as e:
            print(f"âŒ æ·¡å…¥æ•ˆæœåº”ç”¨å¤±è´¥: {e}")
            return audio_input


NODE_CLASS_MAPPINGS = {
    "buding_ConditionalAudio": buding_ConditionalAudio,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "buding_ConditionalAudio": "ğŸµ buding_ConditionalAudio (éŸ³é¢‘å¼€å…³)",
}
