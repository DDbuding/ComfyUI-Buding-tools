"""
buding_è¯»å–TXTæ–‡ä»¶å¢å¼ºç‰ˆ - æ”¯æŒç›®å½•æ‰«æã€å¤šå…³é”®è¯ANDæ¨¡å¼è¿‡æ»¤ã€å¤šç§è¾“å‡ºæ¨¡å¼
ä½œè€…: Buding
åŠŸèƒ½: ä»ç›®å½•ä¸­è¯»å–TXTæ–‡ä»¶ï¼Œæ”¯æŒå•/å¤šå…³é”®è¯è¿‡æ»¤ã€æŒ‰å‰ç¼€æå–å•è¡Œæˆ–å¤šè¡Œæ‰¹é‡è¾“å‡º
     å¤šå…³é”®è¯ç”¨"ã€"åˆ†å‰²ï¼ŒANDé€»è¾‘ï¼ˆæ‰€æœ‰å…³é”®è¯éƒ½å¿…é¡»åœ¨æ–‡ä»¶åä¸­å‡ºç°ï¼‰
"""

import os
import re
from pathlib import Path
from typing import List, Tuple, Dict, Any


def _normalize_newlines(value: str) -> str:
    """ç»Ÿä¸€æ¢è¡Œç¬¦ä¸º \n"""
    return value.replace("\r\n", "\n").replace("\r", "\n")


def _split_filename_keywords(value: str) -> List[str]:
    if not value:
        return []
    parts = [kw.strip() for kw in value.split("ã€") if kw.strip()]
    return parts


def _match_filename_keywords(filename: str, keywords: List[str], match_mode: str) -> bool:
    if not keywords:
        return True
    if match_mode == "ç²¾å‡†åŒ¹é…":
        stem = Path(filename).stem
        return all(kw == stem for kw in keywords)
    return all(kw in filename for kw in keywords)


def _is_separator_line(line: str) -> bool:
    stripped = line.strip()
    if not stripped:
        return False
    parts = stripped.split()
    if len(parts) < 10:
        return False
    return all(part in {"â—†", "â—‡"} for part in parts)


class buding_è¯»å–TXTæ–‡ä»¶å¢å¼ºç‰ˆ:
    """
    å¢å¼ºç‰ˆTXTè¯»å–èŠ‚ç‚¹ï¼šæ”¯æŒç›®å½•æ‰«æã€æ–‡ä»¶è¿‡æ»¤ã€çµæ´»çš„è¾“å‡ºæ¨¡å¼
    
    åŠŸèƒ½ç‰¹ç‚¹ï¼š
    - UTF-8å›ºå®šç¼–ç ï¼Œæ— éœ€ç”¨æˆ·é€‰æ‹©
    - æ”¯æŒç›®å½•æ‰«æï¼Œå¯æ§åˆ¶æ‰«ææ·±åº¦ï¼ˆ0-10å±‚ï¼‰
    - æ–‡ä»¶åå…³é”®è¯è¿‡æ»¤ï¼šæ”¯æŒå•å…³é”®è¯æˆ–å¤šå…³é”®è¯ AND æ¨¡å¼
      * å•å…³é”®è¯ï¼šå¦‚ "åŠ¨æ¼«" åŒ¹é…åŒ…å«"åŠ¨æ¼«"çš„æ–‡ä»¶
      * å¤šå…³é”®è¯ï¼šç”¨"ã€"åˆ†éš”ï¼Œå¦‚ "åŠ¨æ¼«ã€å‰ç¼€" åŒ¹é…åŒæ—¶åŒ…å«"åŠ¨æ¼«"å’Œ"å‰ç¼€"çš„æ–‡ä»¶
      * AND é€»è¾‘ï¼šæ‰€æœ‰å…³é”®è¯éƒ½å¿…é¡»åœ¨æ–‡ä»¶åä¸­ï¼Œé¡ºåºæ— å…³
    - ä¸¤ç§è¾“å‡ºæ¨¡å¼ï¼šå•è¡Œè¾“å‡ºï¼ˆæŒ‰å‰ç¼€æå–ï¼‰ã€å¤šè¡Œè¾“å‡ºï¼ˆæŒ‰è¡Œå·èŒƒå›´ï¼‰
    - è‡ªåŠ¨ç§»é™¤ç©ºè¡Œï¼ˆæ— æ³•å…³é—­ï¼Œé»˜è®¤è¡Œä¸ºï¼‰
    - åŒè¾“å‡ºï¼šæ–‡æœ¬ + ç»Ÿè®¡æ—¥å¿—
    """
    
    CATEGORY = "Buding/TextStory/æ–‡æœ¬å¤„ç†"
    RETURN_TYPES = ("STRING", "STRING", "STRING")
    RETURN_NAMES = ("æ–‡æœ¬è¾“å‡º", "ç»Ÿè®¡æ—¥å¿—", "æ ‡é¢˜è¾“å‡º")
    FUNCTION = "read_advanced"
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "directory_path": ("STRING", {
                    "default": "output/æ–‡æœ¬ä¿å­˜",
                    "tooltip": "æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆç»å¯¹æˆ–ç›¸å¯¹è·¯å¾„ï¼‰"
                }),
                "scan_depth": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 10,
                    "tooltip": "æ‰«ææ·±åº¦ï¼š0=ä»…å½“å‰ç›®å½•ï¼Œ1=å«1å±‚å­ç›®å½•ï¼Œä»¥æ­¤ç±»æ¨"
                }),
                "filename_keyword": ("STRING", {
                    "default": "",
                    "tooltip": "æ–‡ä»¶åå…³é”®è¯è¿‡æ»¤ - å•å…³é”®è¯ï¼šå¦‚ 'ABC' | å¤šå…³é”®è¯ AND æ¨¡å¼ï¼šç”¨'ã€'åˆ†éš”ï¼Œå¦‚ 'ABCã€DEF'ï¼ˆåŒæ—¶åŒ…å«ABCå’ŒDEFï¼‰| ç•™ç©º=æ‰€æœ‰.txtæ–‡ä»¶"
                }),
                "output_mode": (["å•è¡Œè¾“å‡º", "å¤šè¡Œè¾“å‡º"], {
                    "default": "å¤šè¡Œè¾“å‡º",
                    "tooltip": "å•è¡Œè¾“å‡º=æŒ‰å‰ç¼€æå–ï¼›å¤šè¡Œè¾“å‡º=æŒ‰è¡Œå·èŒƒå›´æå–"
                }),
                "prefix_text": ("STRING", {
                    "default": "",
                    "tooltip": "ã€å•è¡Œæ¨¡å¼ã€‘å‰ç¼€æ–‡æœ¬ï¼ˆå¦‚'1.'ï¼‰ï¼Œæå–è¯¥å‰ç¼€åçš„å†…å®¹"
                }),
                "start_line": ("INT", {
                    "default": 0,
                    "min": 0,
                    "tooltip": "ã€å¤šè¡Œæ¨¡å¼ã€‘èµ·å§‹è¡Œç´¢å¼•ï¼ˆ0-basedï¼Œ0è¡¨ç¤ºç¬¬ä¸€è¡Œï¼‰"
                }),
                "max_lines": ("INT", {
                    "default": 100,
                    "min": 1,
                    "max": 10000,
                    "tooltip": "ã€å¤šè¡Œæ¨¡å¼ã€‘æœ€å¤§è¾“å‡ºè¡Œæ•°ï¼ˆ0è¡¨ç¤ºæ— é™åˆ¶ï¼‰"
                }),
                "always_reload": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "å¼€å¯åå§‹ç»ˆé‡æ–°è¯»å–ï¼ˆç»•è¿‡ComfyUIç¼“å­˜ï¼›ç”¨äºä½ ä¿®æ”¹TXTåä¹Ÿèƒ½ç«‹å³ç”Ÿæ•ˆï¼‰"
                }),
                "filter_header_mode": (["ä¸å¼€å¯", "æ¨¡å¼1ï¼šç¬¬ä¸€ä¸ªç©ºè¡Œ", "æ¨¡å¼2ï¼šæœ€åä¸€ä¸ªç©ºè¡Œ"], {
                    "default": "ä¸å¼€å¯",
                    "tooltip": "è¿‡æ»¤æ ‡é¢˜æ¨¡å¼ï¼šæ¨¡å¼1è¯†åˆ«ç¬¬ä¸€ä¸ªç©ºè¡Œä»¥ä¸Šä¸ºæ ‡é¢˜ï¼›æ¨¡å¼2è¯†åˆ«æœ€åä¸€ä¸ªç©ºè¡Œä»¥ä¸Šä¸ºæ ‡é¢˜"
                }),
                "filter_separator": ("BOOLEAN", {"default": False, "tooltip": "å¼€å¯åä»…è¾“å‡ºæœ€åä¸¤æ¡åˆ†éš”è¡Œä¹‹é—´çš„å†…å®¹"}),
                "filename_match_mode": (["åŒ…å«åŒ¹é…", "ç²¾å‡†åŒ¹é…"], {"default": "åŒ…å«åŒ¹é…", "tooltip": "åŒ¹é…æ–¹å¼ï¼šåŒ…å«=åªè¦å…³é”®è¯å‡ºç°å³å¯ï¼›ç²¾å‡†=æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰éœ€å®Œå…¨ç­‰äºå…³é”®è¯"}),
            }
        }

    @classmethod
    def IS_CHANGED(
        cls,
        directory_path: str,
        scan_depth: int,
        filename_keyword: str,
        output_mode: str,
        prefix_text: str,
        start_line: int,
        max_lines: int,
        always_reload: bool = False,
        filter_header_mode: str = "ä¸å¼€å¯",
        filter_separator: bool = False,
        **kwargs,
    ):
        if always_reload:
            return float("nan")

        key_params = {
            "directory_path": directory_path,
            "scan_depth": scan_depth,
            "filename_keyword": filename_keyword,
            "output_mode": output_mode,
            "prefix_text": prefix_text,
            "start_line": start_line,
            "max_lines": max_lines,
            "filter_header_mode": filter_header_mode,
            "filter_separator": filter_separator,
            "filename_match_mode": filename_match_mode,
        }
        return hash(frozenset(key_params.items()))
    
    def _scan_txt_files(self, directory: Path, keyword: str = "", max_depth: int = 0,
                        current_depth: int = 0, match_mode: str = "åŒ…å«åŒ¹é…") -> List[Path]:
        """
        é€’å½’æ‰«æç›®å½•ä¸‹çš„TXTæ–‡ä»¶
        
        Args:
            directory: ç›®å½•è·¯å¾„
            keyword: æ–‡ä»¶åå…³é”®è¯è¿‡æ»¤
            max_depth: æœ€å¤§æ‰«ææ·±åº¦
            current_depth: å½“å‰æ‰«ææ·±åº¦
            
        Returns:
            ç¬¦åˆæ¡ä»¶çš„TXTæ–‡ä»¶åˆ—è¡¨
        """
        files = []
        
        if not directory.exists() or not directory.is_dir():
            return files
        
        try:
            for item in directory.iterdir():
                if item.is_file() and item.suffix.lower() == ".txt":
                    if keyword:
                        keywords = _split_filename_keywords(keyword)
                        if _match_filename_keywords(item.name, keywords, match_mode):
                            files.append(item)
                    else:
                        files.append(item)
                
                elif item.is_dir() and current_depth < max_depth:
                    # é€’å½’æ‰«æå­ç›®å½•
                    sub_files = self._scan_txt_files(item, keyword, max_depth, current_depth + 1)
                    files.extend(sub_files)
        except (PermissionError, OSError) as e:
            pass  # å¿½ç•¥æƒé™é”™è¯¯
        
        return files

    def _read_file_lines(self, file_path: Path, keep_empty_lines: bool = False) -> Tuple[List[str], str]:
        """
        è¯»å–æ–‡ä»¶ï¼Œå¯é€‰æ‹©æ˜¯å¦ä¿ç•™ç©ºè¡Œ
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            keep_empty_lines: æ˜¯å¦ä¿ç•™ç©ºè¡Œ
            
        Returns:
            (è¡Œåˆ—è¡¨, è¯»å–çŠ¶æ€ä¿¡æ¯)
        """
        try:
            content = file_path.read_text(encoding="utf-8")
            normalized = _normalize_newlines(content)
            
            # åˆ†å‰²è¡Œ
            all_lines = normalized.split("\n")
            
            if keep_empty_lines:
                # ä¿ç•™æ‰€æœ‰è¡Œï¼Œåªå»é™¤é¦–å°¾ç©ºç™½
                lines = [line.rstrip() for line in all_lines]
            else:
                # ç§»é™¤ç©ºè¡Œï¼ˆåŸæœ‰é€»è¾‘ï¼‰
                lines = [line.strip() for line in all_lines if line.strip()]
            
            return lines, f"âœ“ è¯»å–æˆåŠŸï¼ˆåŸå§‹{len(all_lines)}è¡Œï¼Œæœ‰æ•ˆ{len(lines)}è¡Œï¼‰"
        except Exception as e:
            return [], f"âœ— è¯»å–å¤±è´¥ï¼š{str(e)}"
    
    def _extract_by_prefix(self, lines: List[str], prefix: str) -> Tuple[str, Dict[str, int]]:
        """
        æŒ‰å‰ç¼€æå–å•è¡Œå†…å®¹
        
        Args:
            lines: è¡Œåˆ—è¡¨
            prefix: å‰ç¼€æ–‡æœ¬ï¼ˆå¦‚"1."ï¼‰
            
        Returns:
            (æå–çš„æ–‡æœ¬, ç»Ÿè®¡ä¿¡æ¯å­—å…¸)
        """
        stats = {
            "scanned_lines": len(lines),
            "matched_lines": 0,
            "output_lines": 0,
        }
        
        if not prefix:
            return "", stats
        
        # æŸ¥æ‰¾ä»¥å‰ç¼€å¼€å¤´çš„è¡Œ
        for line in lines:
            if line.startswith(prefix):
                # æå–å‰ç¼€åçš„æ–‡æœ¬
                content = line[len(prefix):].lstrip()
                stats["matched_lines"] = 1
                stats["output_lines"] = 1
                return content, stats
        
        return "", stats
    
    def _extract_by_range(self, lines: List[str], start_line: int = 0, max_lines: int = 0) -> Tuple[str, Dict[str, int]]:
        """
        æŒ‰è¡Œå·èŒƒå›´æå–å¤šè¡Œå†…å®¹
        
        Args:
            lines: è¡Œåˆ—è¡¨
            start_line: èµ·å§‹è¡Œç´¢å¼•ï¼ˆ0-basedï¼Œ0è¡¨ç¤ºç¬¬ä¸€è¡Œï¼‰
            max_lines: æœ€å¤§è¡Œæ•°ï¼ˆ0è¡¨ç¤ºæ— é™åˆ¶ï¼‰
            
        Returns:
            (æå–çš„æ–‡æœ¬, ç»Ÿè®¡ä¿¡æ¯å­—å…¸)
        """
        stats = {
            "scanned_lines": len(lines),
            "matched_lines": len(lines),
            "output_lines": 0,
            "last_file_name": "",
            "last_file_line": 0
        }
        
        # ä½¿ç”¨ 0-based ç´¢å¼•ç›´æ¥å¤„ç†
        start_idx = max(0, start_line)
        
        # ç¡®å®šç»“æŸä½ç½®
        if max_lines <= 0:
            # æ— é™åˆ¶
            selected_lines = lines[start_idx:]
        else:
            end_idx = start_idx + max_lines
            selected_lines = lines[start_idx:end_idx]
        
        stats["output_lines"] = len(selected_lines)
        output_text = "\n".join(selected_lines)
        
        return output_text, stats
    
    def _filter_header_content_v2(self, dir_path: Path, filename_keyword: str, scan_depth: int,
                                  mode: str, match_mode: str) -> Tuple[List[str], str]:
        """
        è¿‡æ»¤æ ‡é¢˜å†…å®¹ v2ï¼šæ”¯æŒä¸¤ç§æ¨¡å¼
        Mode 1: è¿‡æ»¤æ‰ç¬¬ä¸€ä¸ªç©ºè¡Œä»¥ä¸Šçš„å†…å®¹
        Mode 2: è¿‡æ»¤æ‰æœ€åä¸€ä¸ªç©ºè¡Œä»¥ä¸Šçš„å†…å®¹
        """
        if mode == "ä¸å¼€å¯":
            return None, ""

        filtered_lines = []
        header_lines = []
        
        txt_files = self._scan_txt_files(
            dir_path,
            keyword=filename_keyword.strip(),
            max_depth=scan_depth,
            match_mode=match_mode
        )
        
        for file_path in sorted(txt_files):
            lines, status = self._read_file_lines(file_path, keep_empty_lines=True)
            
            target_idx = -1
            if mode == "æ¨¡å¼1ï¼šç¬¬ä¸€ä¸ªç©ºè¡Œ":
                for i, line in enumerate(lines):
                    if line.strip() == "":
                        target_idx = i
                        break
            elif mode == "æ¨¡å¼2ï¼šæœ€åä¸€ä¸ªç©ºè¡Œ":
                for i in range(len(lines) - 1, -1, -1):
                    if lines[i].strip() == "":
                        target_idx = i
                        break
            
            if target_idx >= 0:
                header_lines.extend(lines[:target_idx])
                content_lines = lines[target_idx + 1:]
            else:
                content_lines = lines
            
            content_lines = [line for line in content_lines if line.strip()]
            filtered_lines.extend(content_lines)
        
        header_content = "\n".join(header_lines).strip()
        return filtered_lines, header_content
    
    def _generate_log(
        self,
        mode: str,
        scanned_count: int,
        stats: Dict[str, Any],
        file_count: int = 1,
        file_names: List[str] | None = None,
        directory: Path | None = None,
    ) -> str:
        """
        ç”Ÿæˆç»Ÿè®¡æ—¥å¿—
        """
        names = file_names or []
        dir_str = str(directory) if directory else ""
        
        # é™åˆ¶æ–‡ä»¶åæ˜¾ç¤ºæ•°é‡
        max_display = 20
        display_names = names[:max_display]
        
        name_list_str = ""
        for i, name in enumerate(display_names):
            name_list_str += f"   {i+1:02d}. {name}\n"
        
        if len(names) > max_display:
            name_list_str += f"   ... (è¿˜æœ‰ {len(names) - max_display} ä¸ªæ–‡ä»¶æœªæ˜¾ç¤º)\n"

        log = (
            f"ğŸ“¥ æ‰¹é‡è¯»å–ç»Ÿè®¡ | âœ… æ–‡ä»¶æ€»æ•°: {file_count}\n"
            f"ğŸ“‚ æ ¹ç›®å½•: {dir_str}\n"
            f"ğŸ“‰ æ•°æ®é‡: æ‰«æ {scanned_count} è¡Œ -> ç­›é€‰è¾“å‡º {stats.get('output_lines', 0)} è¡Œ\n"
            f"----------------------------------------\n"
            f"ğŸ·ï¸ æ–‡ä»¶åæ¸…å• (å‰ {max_display} ä¸ª):\n"
            f"{name_list_str}"
            f"----------------------------------------\n"
            f"ğŸ“ ç»“æŸä½ç½®: {stats.get('last_file_name', 'æœªçŸ¥')}\n"
            f"ğŸ“Œ ç»“æŸè¡Œå·: ç¬¬ {stats.get('last_file_line', 0)} è¡Œ (æ–‡ä»¶å†…è¡Œå·)"
        )
        
        return log
    
    def read_advanced(
        self,
        directory_path: str,
        scan_depth: int,
        filename_keyword: str,
        output_mode: str,
        prefix_text: str,
        start_line: int,
        max_lines: int,
        always_reload: bool,
        filter_header_mode: str = "ä¸å¼€å¯",
        filter_separator: bool = False,
        filename_match_mode: str = "åŒ…å«åŒ¹é…",
    ) -> Tuple[str, str, str]:
        """
        ä¸»å¤„ç†å‡½æ•°
        
        Args:
            directory_path: ç›®å½•è·¯å¾„
            scan_depth: æ‰«ææ·±åº¦
            filename_keyword: æ–‡ä»¶åå…³é”®è¯
            output_mode: è¾“å‡ºæ¨¡å¼
            prefix_text: å‰ç¼€æ–‡æœ¬ï¼ˆå•è¡Œæ¨¡å¼ç”¨ï¼‰
            start_line: å¼€å§‹è¡Œå·ï¼ˆå¤šè¡Œæ¨¡å¼ç”¨ï¼‰
            max_lines: æœ€å¤§è¡Œæ•°ï¼ˆå¤šè¡Œæ¨¡å¼ç”¨ï¼‰
            
        Returns:
            (æ–‡æœ¬è¾“å‡º, ç»Ÿè®¡æ—¥å¿—, æ ‡é¢˜å†…å®¹)
        """
        # æ‰©å±•è·¯å¾„
        dir_path = Path(directory_path).expanduser()
        
        # éªŒè¯ç›®å½•
        if not dir_path.exists():
            return "", f"âŒ é”™è¯¯: ç›®å½•ä¸å­˜åœ¨\nè·¯å¾„: {dir_path}", ""
        
        if not dir_path.is_dir():
            return "", f"âŒ é”™è¯¯: è¾“å…¥çš„ä¸æ˜¯ç›®å½•\nè·¯å¾„: {dir_path}", ""
        
        # æ‰«ææ–‡ä»¶
        txt_files = self._scan_txt_files(
            dir_path,
            keyword=filename_keyword.strip(),
            max_depth=scan_depth,
            match_mode=filename_match_mode
        )
        
        if not txt_files:
            keyword_info = f"ï¼ˆå…³é”®è¯: '{filename_keyword}'ï¼‰" if filename_keyword else ""
            return "", f"âš ï¸  æœªæ‰¾åˆ°åŒ¹é…çš„TXTæ–‡ä»¶\nç›®å½•: {dir_path}\n{keyword_info}", ""
        
        # è¯»å–æ‰€æœ‰æ–‡ä»¶çš„å†…å®¹
        all_lines = []
        file_stats = [] # æ”¹ä¸ºåˆ—è¡¨å­˜å‚¨ (æ–‡ä»¶å, è¡Œæ•°)
        file_names = []
        header_content = ""  # åˆå§‹åŒ–æ ‡é¢˜å†…å®¹
        
        for file_path in sorted(txt_files):
            lines, status = self._read_file_lines(file_path)
            all_lines.extend(lines)
            file_stats.append((file_path.name, len(lines)))
            file_names.append(file_path.name)
        
        total_scanned = sum(count for name, count in file_stats)
        
        # åˆ†éš”è¡Œè¿‡æ»¤é€»è¾‘ (ä¼˜å…ˆçº§æœ€é«˜)
        if filter_separator:
            separator_idxs = [i for i, line in enumerate(all_lines) if _is_separator_line(line)]
            if len(separator_idxs) >= 2:
                start = separator_idxs[-2] + 1
                end = separator_idxs[-1]
                all_lines = all_lines[start:end]
            else:
                all_lines = []

        # è¿‡æ»¤æ ‡é¢˜ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if filter_header_mode != "ä¸å¼€å¯":
            v2_lines, header_content = self._filter_header_content_v2(
                dir_path,
                filename_keyword,
                scan_depth,
                filter_header_mode,
                filename_match_mode,
            )
            if v2_lines is not None:
                all_lines = v2_lines
        else:
            header_content = ""  # æœªå¼€å¯è¿‡æ»¤æ—¶ï¼Œæ ‡é¢˜è¾“å‡ºä¸ºç©º
        
        # æ ¹æ®æ¨¡å¼æå–å†…å®¹
        if output_mode == "å•è¡Œè¾“å‡º":
            if not prefix_text:
                return "", f"âŒ é”™è¯¯: å•è¡Œè¾“å‡ºæ¨¡å¼éœ€è¦æŒ‡å®šå‰ç¼€æ–‡æœ¬", ""
            
            output_text, stats = self._extract_by_prefix(all_lines, prefix_text.strip())
            
            if not output_text:
                return "", (
                    f"âš ï¸  æœªæ‰¾åˆ°åŒ¹é…çš„å‰ç¼€\n"
                    f"å‰ç¼€: '{prefix_text}'\n"
                    f"æ‰«æè¡Œæ•°: {total_scanned}"
                ), ""
            
            # è®¡ç®—å•è¡Œæ¨¡å¼çš„ç»“æŸä½ç½®
            # æ‰¾åˆ°åŒ¹é…è¡Œåœ¨ all_lines ä¸­çš„ç´¢å¼•
            try:
                match_idx = -1
                for i, line in enumerate(all_lines):
                    if line.startswith(prefix_text.strip()):
                        match_idx = i
                        break
                
                if match_idx >= 0:
                    curr_idx = 0
                    for f_name, f_count in file_stats:
                        if curr_idx + f_count > match_idx:
                            stats["last_file_name"] = f_name
                            stats["last_file_line"] = match_idx - curr_idx + 1
                            break
                        curr_idx += f_count
            except: pass

        else:  # å¤šè¡Œè¾“å‡º
            output_text, stats = self._extract_by_range(all_lines, start_line, max_lines)
            if not output_text:
                return "", (
                    f"âš ï¸  è¡Œå·èŒƒå›´æ— æ•ˆæˆ–è¶…å‡ºèŒƒå›´\n"
                    f"èµ·å§‹è¡Œ: {start_line}, æœ€å¤§è¡Œæ•°: {max_lines}\n"
                    f"æ€»è¡Œæ•°: {total_scanned}"
                )
            
            # è®¡ç®—å¤šè¡Œæ¨¡å¼çš„ç»“æŸä½ç½®
            last_idx = start_line + stats["output_lines"] - 1
            if last_idx >= 0 and last_idx < len(all_lines):
                curr_idx = 0
                for f_name, f_count in file_stats:
                    if curr_idx + f_count > last_idx:
                        stats["last_file_name"] = f_name
                        stats["last_file_line"] = last_idx - curr_idx + 1
                        break
                    curr_idx += f_count
        
        # ç”Ÿæˆæ—¥å¿—
        log = self._generate_log(
            output_mode,
            total_scanned,
            stats,
            len(txt_files),
            file_names=file_names,
            directory=dir_path,
        )
        
        return output_text, log, header_content


# èŠ‚ç‚¹æ³¨å†Œä¿¡æ¯
NODE_CLASS_MAPPINGS = {
    "buding_è¯»å–TXTæ–‡ä»¶å¢å¼ºç‰ˆ": buding_è¯»å–TXTæ–‡ä»¶å¢å¼ºç‰ˆ,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "buding_è¯»å–TXTæ–‡ä»¶å¢å¼ºç‰ˆ": "buding_è¯»å–TXTæ–‡ä»¶å¢å¼ºç‰ˆ",
}
