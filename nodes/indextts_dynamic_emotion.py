"""
ğŸ­ IndexTTS Dynamic Emotion - åŠ¨æ€æƒ…ç»ªTTSç”ŸæˆèŠ‚ç‚¹
æ”¯æŒ [è§’è‰²å]<æƒ…ç»ª>å°è¯ æ ¼å¼çš„æ–‡æœ¬è§£æå’Œæƒ…æ„Ÿå‘é‡ç”Ÿæˆ

æ”¾ç½®åœ¨ buding_Tools æ’ä»¶ä¸­ï¼Œç‹¬ç«‹äº IndexTTS æ’ä»¶
é€šè¿‡å­—ç¬¦ä¸²ç±»å‹åŒ¹é… "EASY_INDEXTTS_MODEL" ä¸ IndexTTS æ’ä»¶è¿æ¥
"""
import re
import json
import numpy as np
import torch
import comfy.utils

# æƒ…ç»ªåç§°åˆ°å‘é‡ç´¢å¼•çš„æ˜ å°„
# å‘é‡æ ¼å¼: [Happy, Angry, Sad, Fear, Hate, Low, Surprise, Neutral]
EMOTION_INDEX = {
    # ä¸­æ–‡æƒ…ç»ªå
    "å¼€å¿ƒ": 0, "é«˜å…´": 0, "å¿«ä¹": 0, "å–œæ‚¦": 0,
    "æ„¤æ€’": 1, "ç”Ÿæ°”": 1, "æ€’": 1,
    "æ‚²ä¼¤": 2, "éš¾è¿‡": 2, "ä¼¤å¿ƒ": 2, "æ‚²": 2,
    "ææƒ§": 3, "å®³æ€•": 3, "æƒŠæ": 3,
    "åŒæ¶": 4, "è®¨åŒ": 4, "æ¶å¿ƒ": 4, "å«Œå¼ƒ": 4,
    "ä½è½": 5, "æ²®ä¸§": 5, "æ¶ˆæ²‰": 5,
    "æƒŠè®¶": 6, "åƒæƒŠ": 6, "éœ‡æƒŠ": 6,
    "å¹³é™": 7, "ä¸­æ€§": 7, "å¹³æ·¡": 7,
    # è‹±æ–‡æƒ…ç»ªå
    "happy": 0, "joy": 0, "pleased": 0,
    "angry": 1, "anger": 1, "mad": 1,
    "sad": 2, "sadness": 2, "sorrow": 2,
    "fear": 3, "scared": 3, "afraid": 3,
    "hate": 4, "disgust": 4, "dislike": 4,
    "low": 5, "down": 5, "depressed": 5,
    "surprise": 6, "surprised": 6, "shocked": 6,
    "neutral": 7, "calm": 7, "normal": 7,
}

# é»˜è®¤æƒ…ç»ªé¢„è®¾ï¼ˆåŸºå‡†å€¼0.37ï¼ŒNeutralç»´åº¦ä¿æŒ0ï¼‰
DEFAULT_EMOTION_PRESETS = {
    "æ— æƒ…ç»ª": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    "å¼€å¿ƒ": [0.37, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    "æ„¤æ€’": [0.0, 0.37, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    "æ‚²ä¼¤": [0.0, 0.0, 0.37, 0.0, 0.0, 0.0, 0.0, 0.0],
    "ææƒ§": [0.0, 0.0, 0.0, 0.37, 0.0, 0.0, 0.0, 0.0],
    "åŒæ¶": [0.0, 0.0, 0.0, 0.0, 0.37, 0.0, 0.0, 0.0],
    "ä½è½": [0.0, 0.0, 0.0, 0.0, 0.0, 0.37, 0.0, 0.0],
    "æƒŠè®¶": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37, 0.0],
    "å¹³é™": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
}


def process_audio_input(audio):
    """å¤„ç†éŸ³é¢‘è¾“å…¥ï¼Œè½¬æ¢ä¸º (wave, sr) æ ¼å¼"""
    if isinstance(audio, dict) and "waveform" in audio and "sample_rate" in audio:
        wave = audio["waveform"]
        sr = int(audio["sample_rate"])
        if isinstance(wave, torch.Tensor):
            if wave.dim() == 3:
                wave = wave[0, 0].detach().cpu().numpy()
            elif wave.dim() == 1:
                wave = wave.detach().cpu().numpy()
            else:
                wave = wave.flatten().detach().cpu().numpy()
        elif isinstance(wave, np.ndarray):
            if wave.ndim == 3:
                wave = wave[0, 0]
            elif wave.ndim == 2:
                wave = wave[0]
        return wave.astype(np.float32), sr
    elif isinstance(audio, tuple) and len(audio) == 2:
        wave, sr = audio
        if isinstance(wave, torch.Tensor):
            wave = wave.detach().cpu().numpy()
        return wave.astype(np.float32), int(sr)
    else:
        raise ValueError("AUDIO input must be ComfyUI dict or (wave, sr)")


def load_custom_presets(custom_json: str):
    """è§£æç”¨æˆ·æä¾›çš„è‡ªå®šä¹‰é¢„è®¾JSONï¼Œè¿”å›æœ‰æ•ˆé¢„è®¾å’Œè­¦å‘Šä¿¡æ¯ã€‚"""
    presets = {}
    warnings = []
    if not custom_json or not custom_json.strip():
        return presets, warnings
    try:
        data = json.loads(custom_json)
        if not isinstance(data, dict):
            warnings.append("custom_presets_json éœ€è¦æ˜¯å¯¹è±¡ç±»å‹ï¼Œä¾‹å¦‚ {\"å¼€å¿ƒ\": [0.37, ...]}")
            return presets, warnings
        for key, value in data.items():
            if not isinstance(value, (list, tuple)) or len(value) != 8:
                warnings.append(f"é¢„è®¾ {key} å¿…é¡»æ˜¯é•¿åº¦ä¸º8çš„æ•°ç»„ï¼Œå·²è·³è¿‡")
                continue
            try:
                vec = [float(v) for v in value]
            except Exception:
                warnings.append(f"é¢„è®¾ {key} åŒ…å«éæ•°å­—å†…å®¹ï¼Œå·²è·³è¿‡")
                continue
            vec[7] = 0.0  # Neutralç»´åº¦å¼ºåˆ¶ä¸º0
            presets[key] = vec
    except Exception as e:
        warnings.append(f"è§£æ custom_presets_json å¤±è´¥: {e}")
    return presets, warnings


def resolve_emotion_vector(tag: str, presets: dict, base_value: float, intensity_scale: float, custom_keys: set):
    """æ ¹æ®æ ‡ç­¾è§£ææƒ…ç»ªå‘é‡ï¼Œæ”¯æŒé¢„è®¾ã€è‡ªå®šä¹‰åç§°å’Œç»„åˆæ ‡ç­¾ã€‚"""
    tag = (tag or "").strip()
    base_zero = [0.0] * 8
    if tag == "" or tag == "æ— æƒ…ç»ª":
        return base_zero, "none", None, False

    # ç›´æ¥å‘½ä¸­é¢„è®¾ï¼ˆåŒ…å«è‡ªå®šä¹‰ï¼‰
    if tag in presets:
        vec = presets[tag]
        vec = [v * intensity_scale for v in vec]
        vec[7] = 0.0
        source = "custom_preset" if tag in custom_keys else "default_preset"
        return vec, source, None, True

    warnings = []
    emotion_vector = [0.0] * 8
    recognized = False

    # ç»„åˆè§£æï¼šæƒ…ç»ªå + å¯é€‰å¼ºåº¦ï¼Œæ”¯æŒ '+' è¿æ¥
    parts = tag.split("+")
    for part in parts:
        part = part.strip()
        if not part:
            continue
        match = re.match(r"([a-zA-Z\u4e00-\u9fff:_\-]+)([\d.]*)", part)
        if not match:
            warnings.append(f"æœªè¯†åˆ«çš„æƒ…ç»ªç‰‡æ®µ: {part}")
            continue
        name = match.group(1)
        value_str = match.group(2)

        if name in presets:
            vec = presets[name]
            emotion_vector = [max(a, b) for a, b in zip(emotion_vector, vec)]
            recognized = True
            continue

        name_key = name.lower()
        if name_key in EMOTION_INDEX:
            idx = EMOTION_INDEX[name_key]
            try:
                val = float(value_str) if value_str else base_value
            except Exception:
                val = base_value
            emotion_vector[idx] = max(emotion_vector[idx], val)
            recognized = True
        else:
            warnings.append(f"æœªè¯†åˆ«çš„æƒ…ç»ªåç§°: {name}")

    if not recognized:
        return base_zero, "unrecognized", warnings or ["æ ‡ç­¾æœªè¢«è¯†åˆ«ï¼Œå·²ä½¿ç”¨0å‘é‡"], False

    emotion_vector = [v * intensity_scale for v in emotion_vector]
    emotion_vector[7] = 0.0
    return emotion_vector, "parsed", warnings if warnings else None, True


def parse_dynamic_text(text: str):
    """
    è§£æåŠ¨æ€æƒ…ç»ªæ–‡æœ¬æ ¼å¼
    æ”¯æŒæ ¼å¼: [è§’è‰²å]<æƒ…ç»ª>å°è¯
    """
    segments = []
    lines = text.strip().split('\n')
    for i, line in enumerate(lines):
        raw_line = line
        line = line.strip()
        if not line:
            continue

        pause_match = re.match(r'^-(\d+(?:\.\d+)?)s-$', line)
        if pause_match:
            pause_duration = float(pause_match.group(1))
            segments.append({
                "type": "pause",
                "duration": pause_duration,
                "src_line": i + 1
            })
            continue

        pattern = r'^\[([^\]]+)\](?:<([^>]*)>)?(.*)$'
        match = re.match(pattern, line)

        if match:
            role_name = match.group(1).strip()
            emotion_tag = match.group(2) or ""
            dialog_text = match.group(3).strip()
            segments.append({
                "type": "dialog",
                "role": role_name,
                "emotion_tag": emotion_tag,
                "text": dialog_text,
                "src_line": i + 1
            })
        else:
            segments.append({
                "type": "dialog",
                "role": "__DEFAULT__",
                "emotion_tag": "",
                "text": line,
                "src_line": i + 1
            })
    return segments


def _parse_name_list(raw: str):
    return {name.strip() for name in (raw or "").split("ã€") if name.strip()}


def _parse_line_selectors(raw: str):
    selected = set()
    warnings = []
    for token in [t.strip() for t in (raw or "").split("ã€") if t.strip()]:
        if "-" in token:
            try:
                start_str, end_str = token.split("-", 1)
                start = int(start_str)
                end = int(end_str)
                if start > end:
                    start, end = end, start
                selected.update(range(start, end + 1))
            except Exception:
                warnings.append(f"æ— æ³•è§£ææ®µè½èŒƒå›´: {token}")
        else:
            try:
                selected.add(int(token))
            except Exception:
                warnings.append(f"æ— æ³•è§£ææ®µè½ç¼–å·: {token}")
    return selected, warnings


class buding_IndexTTSDynamicEmotion:
    """
    ğŸ­ IndexTTS Dynamic Emotion - åŠ¨æ€æƒ…ç»ªTTSç”Ÿæˆ
    
    æ”¯æŒæ ¼å¼: [è§’è‰²å]<æƒ…ç»ª>å°è¯
    - è§’è‰²å: ä¸ ROLE_AUDIOS å­—å…¸ä¸­çš„é”®åŒ¹é…
    - æƒ…ç»ª: æ”¯æŒä¸­è‹±æ–‡æƒ…ç»ªåï¼Œå¯ç»„åˆï¼Œå¯è®¾å¼ºåº¦
    - å°è¯: è¦åˆæˆçš„æ–‡æœ¬å†…å®¹
    
    ç¤ºä¾‹:
    [æ—ç™½]<å¹³é™>è¿™æ˜¯ä¸€ä¸ªç¾å¥½çš„æ—©æ™¨ã€‚
    [å°æ˜]<å¼€å¿ƒ>ä»Šå¤©å¤©æ°”çœŸä¸é”™ï¼
    [å°çº¢]<æ„¤æ€’0.8+æƒŠè®¶0.3>ä½ æ€ä¹ˆåˆè¿Ÿåˆ°äº†ï¼Ÿ
    -2s-
    [å°æ˜]<æ‚²ä¼¤>å¯¹ä¸èµ·...
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "indextts_model": ("EASY_INDEXTTS_MODEL", {
                    "tooltip": "IndexTTSæ¨¡å‹åŠ è½½å™¨è¾“å‡ºçš„æ¨¡å‹å¥æŸ„"
                }),
                "role_audios": ("ROLE_AUDIOS", {
                    "tooltip": "æ‰¹é‡è§’è‰²éŸ³é¢‘è¾“å‡ºçš„è§’è‰²å­—å…¸ï¼Œé”®=è§’è‰²åï¼Œå€¼=å‚è€ƒéŸ³é¢‘"
                }),
                "default_role": ("STRING", {
                    "default": "æ—ç™½",
                    "multiline": False,
                    "tooltip": "é»˜è®¤è§’è‰²åï¼šå½“æ–‡æœ¬æœªæŒ‡å®šè§’è‰²æˆ–è§’è‰²æœªæ‰¾åˆ°æ—¶ä½¿ç”¨"
                }),
                "text": ("STRING", {
                    "multiline": True,
                    "default": "[æ—ç™½]<å¹³é™>è¿™æ˜¯ç¤ºä¾‹æ–‡æœ¬ã€‚\n[è§’è‰²A]<å¼€å¿ƒ>ä½ å¥½ï¼",
                    "tooltip": "æŒ‰è¡Œè¾“å…¥ï¼š[è§’è‰²]<æƒ…ç»ª>å°è¯ï¼›åœé¡¿å†™ -1.5s-"
                }),
                "base_emotion": ("FLOAT", {
                    "default": 0.37,
                    "min": 0.1,
                    "max": 1.4,
                    "step": 0.01,
                    "display": "slider",
                    "tooltip": "åŸºç¡€æƒ…ç»ªå¼ºåº¦(å®‰å…¨å€¼)ï¼Œé…åˆæƒ…ç»ªåæˆ–è§£æç”¨"
                }),
                "emotion_intensity": ("FLOAT", {
                    "default": 1.0,
                    "min": 0.1,
                    "max": 4.0,
                    "step": 0.05,
                    "display": "slider",
                    "tooltip": "å…¨å±€æƒ…ç»ªå¼ºåº¦å€æ•°(0.5å†…æ•›,1é»˜è®¤,2-3å¼ºçƒˆ)"
                }),
                "emo_weight": ("FLOAT", {
                    "default": 0.6,
                    "min": 0.0,
                    "max": 1.6,
                    "step": 0.05,
                    "display": "slider",
                    "tooltip": "IndexTTSæƒ…ç»ªå½±å“æƒé‡(0=å¿½ç•¥æƒ…ç»ª,0.6ä¿å®ˆæ¨è,1.6æå¼º)"
                }),
                "default_emotion": ("STRING", {
                    "default": "æ— æƒ…ç»ª",
                    "multiline": False,
                    "tooltip": "æœªæ ‡æ³¨è¡Œä½¿ç”¨çš„æƒ…ç»ª(ä¾‹å¦‚ æ— æƒ…ç»ª/å¼€å¿ƒ/æ‚²ä¼¤/è‡ªå®šä¹‰é¢„è®¾å)"
                }),
                "include_roles": ("STRING", {
                    "default": "",
                    "multiline": False,
                    "tooltip": "ä»…ç”ŸæˆæŒ‡å®šè§’è‰²ï¼Œå¤šä¸ªè§’è‰²ç”¨â€œã€â€åˆ†éš”ï¼›ç•™ç©ºè¡¨ç¤ºå…¨éƒ¨"
                }),
                "exclude_roles": ("STRING", {
                    "default": "",
                    "multiline": False,
                    "tooltip": "æ’é™¤è§’è‰²ï¼Œå¤šä¸ªè§’è‰²ç”¨â€œã€â€åˆ†éš”ï¼›ä¼˜å…ˆçº§é«˜äºæŒ‡å®šè§’è‰²"
                }),
                "include_segments": ("STRING", {
                    "default": "",
                    "multiline": False,
                    "tooltip": "ä»…ç”ŸæˆæŒ‡å®šæ®µè½ï¼Œæ”¯æŒé€—å·æ ¼å¼å¦‚ 1ã€3ã€5 æˆ–èŒƒå›´ 3-6"
                }),
                "custom_presets_json": ("STRING", {
                    "multiline": True,
                    "default": "",
                    "tooltip": "è‡ªå®šä¹‰æƒ…ç»ªé¢„è®¾JSONï¼Œé”®=æƒ…ç»ªåï¼Œå€¼=8ç»´æ•°ç»„ï¼›Neutralç»´åº¦ä¼šå¼ºåˆ¶ä¸º0"
                }),
                "sampling_preset": (["è‡ªå®šä¹‰", "å¹³è¡¡", "ç¨³å®š", "åˆ›æ„", "æé€Ÿ"], {
                    "default": "è‡ªå®šä¹‰",
                    "tooltip": "é‡‡æ ·é¢„è®¾ï¼šå¹³è¡¡/ç¨³å®š/åˆ›æ„/æé€Ÿï¼Œä¸€é”®è¦†ç›–é‡‡æ ·å‚æ•°ï¼›è‡ªå®šä¹‰åˆ™ä½¿ç”¨ä¸‹æ–¹æ‰‹åŠ¨å‚æ•°"
                }),
                "unload_model": ("BOOLEAN", {"default": False, "tooltip": "ç”Ÿæˆåæ˜¯å¦å¸è½½æ¨¡å‹ä»¥çœæ˜¾å­˜ï¼ˆé¢‘ç¹ç”Ÿæˆå»ºè®®å…³é—­ä»¥å…åå¤åŠ è½½ï¼‰"}),
                "do_sample": ("BOOLEAN", {"default": True, "tooltip": "é‡‡æ ·å¼€å…³ï¼šå¼€=é‡‡æ ·(é…åˆtemp/top_p)ï¼Œå…³=è´ªå¿ƒ/beamä¸ºä¸»æ›´ç¨³"}),
                "temperature": ("FLOAT", {"default": 0.8, "min": 0.1, "max": 2.0, "step": 0.05, "tooltip": "é‡‡æ ·æ¸©åº¦ï¼šå°=ç¨³å®š, å¤§=å¤šæ ·"}),
                "top_p": ("FLOAT", {"default": 0.9, "min": 0.0, "max": 1.0, "step": 0.01, "tooltip": "æ ¸é‡‡æ ·é˜ˆå€¼ï¼š0.9é»˜è®¤ï¼Œä½å€¼æ›´ç¨³"}),
                "top_k": ("INT", {"default": 30, "min": 0, "max": 100, "step": 1, "tooltip": "top-ké‡‡æ ·ï¼š0å…³é—­, 30é»˜è®¤"}),
                "num_beams": ("INT", {"default": 3, "min": 1, "max": 10, "step": 1, "tooltip": "beam searchæ•°é‡ï¼šå¤§=æ›´ç¨³ä½†æ›´æ…¢"}),
                "repetition_penalty": ("FLOAT", {"default": 10.0, "min": 1.0, "max": 10.0, "step": 0.1, "tooltip": "é‡å¤æƒ©ç½šï¼šå¤§å€¼é¿å…å•°å—¦"}),
                "length_penalty": ("FLOAT", {"default": 0.0, "min": -2.0, "max": 2.0, "step": 0.1, "tooltip": "é•¿åº¦æƒ©ç½šï¼š>0æ›´çŸ­ï¼Œ<0æ›´é•¿"}),
                "max_mel_tokens": ("INT", {
                    "default": 1815,
                    "min": 50,
                    "max": 1815,
                    "step": 5,
                    "tooltip": "Melé•¿åº¦ä¸Šé™ï¼š1815ä¸ºå®˜æ–¹ä¸Šé™ï¼›é•¿å¥/æ­Œæ›²å¯ 1500-1815ï¼›è¿½æ±‚é€Ÿåº¦å¯ 800-1200ï¼ˆå¯èƒ½æˆªæ–­ï¼‰"
                }),
                "max_tokens_per_sentence": ("INT", {
                    "default": 120,
                    "min": 0,
                    "max": 600,
                    "step": 5,
                    "tooltip": "æ–‡æœ¬tokenä¸Šé™ï¼šæ™®é€šå¥å­ 80-150ï¼›é•¿å¥å¯¹ç™½ 150-300ï¼›æé•¿å°è¯å¯ 300-600ï¼Œä½†é€Ÿåº¦ä¼šä¸‹é™"
                }),
                "speech_speed": ("FLOAT", {"default": 1.0, "min": 0.5, "max": 2.0, "step": 0.05, "tooltip": "è¯­é€Ÿå€ç‡ï¼š0.8æ…¢è¯­é€Ÿï¼Œ1.0æ­£å¸¸ï¼Œ1.2å¿«"}),
                "seed": ("INT", {"default": 0, "min": 0, "max": 0xffffffff, "tooltip": "éšæœºç§å­ï¼š0ä¸ºéšæœºï¼Œå¯å¡«å›ºå®šå€¼å¤ç°ç»“æœ"}),
            }
        }

    RETURN_TYPES = ("AUDIO", "AUDIOS", "INT", "STRING", "STRING")
    RETURN_NAMES = ("merged_audio", "audio_list", "seed", "subtitle", "emotion_log")
    FUNCTION = "generate"
    CATEGORY = "buding_Tools/TTS"
    
    def generate(self, indextts_model, role_audios, default_role, text, base_emotion,
                 emotion_intensity, emo_weight, default_emotion, include_roles,
                 exclude_roles, include_segments, custom_presets_json,
                 sampling_preset, unload_model, do_sample, temperature, top_p,
                 top_k, num_beams, repetition_penalty, length_penalty,
                 max_mel_tokens, max_tokens_per_sentence, speech_speed,
                 seed):

        segments = parse_dynamic_text(text)

        if not segments:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ–‡æœ¬æ®µè½")

        include_roles_set = _parse_name_list(include_roles)
        exclude_roles_set = _parse_name_list(exclude_roles)
        include_lines_set, selector_warnings = _parse_line_selectors(include_segments)

        specified_mode_skip_pause = bool(include_roles_set) or bool(include_lines_set)

        # è§’è‰²åŒ¹é…ç»Ÿè®¡ï¼ˆç”¨äºæ—¥å¿—/æŠ¥å‘Šï¼‰ï¼šåœ¨â€œæ®µè½ç­›é€‰(include_segments)â€åã€åœ¨â€œè§’è‰²è¿‡æ»¤(include/exclude)â€å‰ç»Ÿè®¡
        roles_in_text = set()
        for seg in segments:
            src_line = seg.get("src_line", 0)
            if include_lines_set and src_line not in include_lines_set:
                continue
            if seg.get("type") != "dialog":
                continue
            role_name = seg.get("role") or "__DEFAULT__"
            resolved_role = default_role if role_name == "__DEFAULT__" else role_name
            roles_in_text.add(resolved_role)

        matched_roles = {r for r in roles_in_text if r in role_audios}
        missing_roles = sorted([r for r in roles_in_text if r not in role_audios])

        if roles_in_text:
            print(
                f"è§’è‰²åŒ¹é…(æ®µè½ç­›é€‰å): {len(matched_roles)}/{len(roles_in_text)} | "
                f"åŒ¹é…è§’è‰²: {', '.join(sorted(matched_roles)) if matched_roles else 'æ— '}"
            )
            if missing_roles:
                print(f"æœªæ‰¾åˆ°è§’è‰²(å°†å›é€€é»˜è®¤è§’è‰²): {', '.join(missing_roles)}")

        custom_presets, preset_warnings = load_custom_presets(custom_presets_json)
        presets = {**DEFAULT_EMOTION_PRESETS, **custom_presets}
        custom_keys = set(custom_presets.keys())

        stats = {
            "dialog_count": 0,
            "pause_count": 0,
            "custom_hit": 0,
            "default_hit": 0,
            "raw_voice_hit": 0,
            "matched_roles": matched_roles,
            "loaded_roles": set(),
            "unrecognized": [],
            "warnings": preset_warnings[:] + selector_warnings,
            "total_time": 0.0,
        }

        filtered_segments = []
        for seg in segments:
            src_line = seg.get("src_line", 0)
            if include_lines_set and src_line not in include_lines_set:
                continue

            if seg["type"] != "dialog":
                if specified_mode_skip_pause and seg.get("type") == "pause":
                    continue
                filtered_segments.append(seg)
                continue

            role_name = seg.get("role") or "__DEFAULT__"
            resolved_role = default_role if role_name == "__DEFAULT__" else role_name

            if exclude_roles_set and resolved_role in exclude_roles_set:
                continue
            if include_roles_set and resolved_role not in include_roles_set:
                continue

            seg_copy = dict(seg)
            seg_copy["role"] = resolved_role
            filtered_segments.append(seg_copy)

        segments = filtered_segments

        if not segments:
            raise ValueError("è¿‡æ»¤åæ²¡æœ‰å¯ç”Ÿæˆçš„æ®µè½")

        # é¢„åŠ è½½/ç¼“å­˜ï¼šä»…å¯¹æœ¬æ¬¡å®é™…ä¼šç”¨åˆ°çš„è§’è‰²åšä¸€æ¬¡ process_audio_inputï¼Œå¹¶ç¼“å­˜åˆ°å†…å­˜ï¼ˆCPUï¼‰
        roles_to_load = set()
        for seg in segments:
            if seg.get("type") != "dialog":
                continue
            role_name = seg.get("role") or default_role
            # è‹¥ç¼ºå¤±åˆ™åœ¨ç”Ÿæˆæ—¶å›é€€ default_roleï¼Œè¿™é‡Œä¹Ÿå°½é‡å¯¹ default_role åšé¢„åŠ è½½
            if role_name in role_audios:
                roles_to_load.add(role_name)
            else:
                roles_to_load.add(default_role)

        if include_roles_set or exclude_roles_set or include_lines_set:
            filter_notes = []
            if include_roles_set:
                filter_notes.append(f"ä»…è§’è‰²={ 'ã€'.join(sorted(include_roles_set)) }")
            if exclude_roles_set:
                filter_notes.append(f"æ’é™¤è§’è‰²={ 'ã€'.join(sorted(exclude_roles_set)) }")
            if include_lines_set:
                filter_notes.append(f"æ®µè½={ 'ã€'.join(str(i) for i in sorted(include_lines_set)) }")
            print(f"ç­›é€‰æ¡ä»¶: {' | '.join(filter_notes)}")

        print(
            f"å®é™…å°†åŠ è½½è§’è‰²({len(roles_to_load)}): "
            f"{', '.join(sorted(roles_to_load)) if roles_to_load else 'æ— '}"
        )

        ref_audio_cache = {}
        for role_name in sorted(roles_to_load):
            if role_name not in role_audios:
                raise ValueError(f"è§’è‰² '{role_name}' ä¸å­˜åœ¨äº role_audiosï¼Œä¸”æ— æ³•å›é€€")
            print(f"é¢„åŠ è½½è§’è‰²éŸ³è‰²: {role_name}")
            ref_audio_cache[role_name] = process_audio_input(role_audios[role_name])

        stats["loaded_roles"] = set(ref_audio_cache.keys())

        sampling_presets = {
            "å¹³è¡¡": {
                "temperature": 0.8,
                "top_p": 0.9,
                "top_k": 30,
                "num_beams": 3,
                "repetition_penalty": 10.0,
                "length_penalty": 0.0,
                "note": "é€šç”¨å¹³è¡¡"
            },
            "ç¨³å®š": {
                "temperature": 0.6,
                "top_p": 0.85,
                "top_k": 20,
                "num_beams": 4,
                "repetition_penalty": 8.0,
                "length_penalty": 0.2,
                "note": "æ›´ä¿å®ˆé˜²è·‘å"
            },
            "åˆ›æ„": {
                "temperature": 1.0,
                "top_p": 0.95,
                "top_k": 50,
                "num_beams": 1,
                "repetition_penalty": 7.0,
                "length_penalty": -0.1,
                "note": "æ›´å¤šæ ·ä½†ç•¥ä¸ç¨³"
            },
            "æé€Ÿ": {
                "temperature": 0.8,
                "top_p": 0.9,
                "top_k": 0,
                "num_beams": 1,
                "repetition_penalty": 8.0,
                "length_penalty": 0.0,
                "note": "å°‘beamå°‘top_kæé€Ÿ"
            }
        }

        # é‡‡æ ·å‚æ•°å®é™…å€¼ï¼ˆå¯èƒ½è¢«é¢„è®¾è¦†ç›–ï¼‰
        temp_val = temperature
        top_p_val = top_p
        top_k_val = top_k
        num_beams_val = num_beams
        rep_val = repetition_penalty
        len_pen_val = length_penalty

        preset_note = "è‡ªå®šä¹‰"
        if sampling_preset in sampling_presets:
            p = sampling_presets[sampling_preset]
            temp_val = p["temperature"]
            top_p_val = p["top_p"]
            top_k_val = p["top_k"]
            num_beams_val = p["num_beams"]
            rep_val = p["repetition_penalty"]
            len_pen_val = p["length_penalty"]
            preset_note = f"{sampling_preset}({p['note']})"

        parse_info_lines = []
        detail_rows = []
        all_waves = []
        all_audios = []
        all_subtitles = []
        current_time = 0.0
        current_sr = None

        pbar = comfy.utils.ProgressBar(len(segments))

        dialog_counter = 0

        import time
        start_time = time.time()


        for seg_idx, segment in enumerate(segments):
            seg_start_time = time.time()
            src_line = segment.get("src_line", seg_idx + 1)
            if segment["type"] == "pause":
                if specified_mode_skip_pause:
                    pbar.update(1)
                    continue
                stats["pause_count"] += 1
                pause_duration = segment["duration"]
                sample_rate = current_sr if current_sr else 22050
                silence_samples = int(pause_duration * sample_rate)
                silence_wave = np.zeros(silence_samples, dtype=np.float32)
                silence_tensor = torch.from_numpy(silence_wave).unsqueeze(0).unsqueeze(0)

                all_waves.append(silence_wave)
                all_audios.append({
                    "waveform": silence_tensor,
                    "sample_rate": int(sample_rate),
                    "type": "pause",
                    "index": src_line,
                    "duration": pause_duration
                })

                all_subtitles.append({
                    "id": "pause",
                    "index": src_line,
                    "å­—å¹•": f"[åœé¡¿ {pause_duration}ç§’]",
                    "start": round(current_time, 2),
                    "end": round(current_time + pause_duration, 2)
                })
                current_time += pause_duration
                pbar.update(1)
                continue

            role_name = segment["role"]
            dialog_text = segment["text"]
            raw_tag = (segment.get("emotion_tag") or "").strip()
            effective_tag = raw_tag if raw_tag else default_emotion

            if role_name not in role_audios:
                print(f"è­¦å‘Š: è§’è‰² '{role_name}' æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤è§’è‰² '{default_role}'")
                role_name = default_role

            if role_name is None:
                raise ValueError("æ²¡æœ‰å¯ç”¨çš„è§’è‰²éŸ³é¢‘")

            ref_audio = ref_audio_cache.get(role_name)
            if ref_audio is None:
                # ç†è®ºä¸Šä¸ä¼šå‘ç”Ÿï¼ˆå·²é¢„åŠ è½½ï¼‰ï¼Œä¿åº•å…œåº•
                ref_audio = process_audio_input(role_audios[role_name])
                ref_audio_cache[role_name] = ref_audio

            emo_vector, source, warn, recognized = resolve_emotion_vector(
                effective_tag, presets, base_emotion, emotion_intensity, custom_keys
            )

            if source == "custom_preset":
                stats["custom_hit"] += 1
            elif source == "default_preset":
                stats["default_hit"] += 1
            elif source == "unrecognized":
                stats["unrecognized"].append(effective_tag)

            if warn:
                stats["warnings"].extend(warn if isinstance(warn, list) else [warn])

            has_emotion = any(v > 0 for v in emo_vector)
            emo_vector_param = emo_vector if has_emotion else None

            if emo_vector_param is None:
                stats["raw_voice_hit"] += 1

            print(f"ç”Ÿæˆ: è§’è‰²={role_name}, æƒ…ç»ª={effective_tag if has_emotion else 'é»˜è®¤'}, æ¥æº={source}, æ–‡æœ¬={dialog_text[:30]}...")

            sr, wave, sub = indextts_model.generate(
                text=dialog_text,
                reference_audio=ref_audio,
                mode="Auto",
                do_sample=do_sample,
                temperature=temp_val,
                top_p=top_p_val,
                top_k=top_k_val,
                num_beams=num_beams_val,
                repetition_penalty=rep_val,
                length_penalty=len_pen_val,
                max_mel_tokens=max_mel_tokens,
                max_tokens_per_sentence=max_tokens_per_sentence,
                speech_speed=speech_speed,
                emo_text=None,
                emo_ref_audio=None,
                emo_vector=emo_vector_param,
                emo_weight=emo_weight,
                seed=seed,
                return_subtitles=True,
                use_random=False,
                use_qwen=False
            )

            if current_sr is None:
                current_sr = sr

            wave_np = np.asarray(wave, dtype=np.float32)
            all_waves.append(wave_np)

            dialog_counter += 1
            segment_duration = len(wave_np) / float(sr)
            stats["total_time"] += segment_duration

            audio_tensor = torch.from_numpy(wave_np).unsqueeze(0).unsqueeze(0)
            all_audios.append({
                "waveform": audio_tensor,
                "sample_rate": int(sr),
                "type": "dialog",
                "index": src_line,
                "role": role_name,
                "text": dialog_text,
                "emotion_tag": effective_tag,
                "emotion_vector": emo_vector
            })

            if sub:
                try:
                    sub_data = json.loads(sub)
                    for item in sub_data:
                        item["id"] = role_name
                        item["index"] = src_line
                        item["start"] = round(current_time + item.get("start", 0), 2)
                        item["end"] = round(current_time + item.get("end", segment_duration), 2)
                    all_subtitles.extend(sub_data)
                except Exception:
                    all_subtitles.append({
                        "id": role_name,
                        "index": src_line,
                        "å­—å¹•": dialog_text,
                        "start": round(current_time, 2),
                        "end": round(current_time + segment_duration, 2)
                    })
            else:
                all_subtitles.append({
                    "id": role_name,
                    "index": src_line,
                    "å­—å¹•": dialog_text,
                    "start": round(current_time, 2),
                    "end": round(current_time + segment_duration, 2)
                })

            vector_preview = ", ".join([f"{v:.2f}" for v in emo_vector])
            icon = "âœ…" if source in ("default_preset", "custom_preset", "parsed") else "âš ï¸"
            tag_display = effective_tag or "æ— "
            detail_rows.append(
                f"{src_line:>3} | {icon:^4} | {role_name:<8} | {tag_display:<10} | [{vector_preview}] | {segment_duration:>6.2f}s"
            )
            detail_rows.append(f"      {dialog_text}")
            detail_rows.append("")

            current_time += segment_duration
            stats["dialog_count"] += 1
            seg_end_time = time.time()
            print(f"æ®µè½ {src_line} å¤„ç†æ—¶é—´: {seg_end_time - seg_start_time:.2f}ç§’")
            pbar.update(1)

        loaded_roles_list = sorted(stats["loaded_roles"]) if isinstance(stats.get("loaded_roles"), set) else []
        print(
            f"å®é™…åŠ è½½è§’è‰²æ•°é‡: {len(loaded_roles_list)} | "
            f"åŠ è½½è§’è‰²: {', '.join(loaded_roles_list) if loaded_roles_list else 'æ— '}"
        )

        final_wave = np.concatenate(all_waves) if all_waves else np.array([])
        merged_tensor = torch.from_numpy(final_wave.astype(np.float32)).unsqueeze(0).unsqueeze(0)
        merged_audio = {"waveform": merged_tensor, "sample_rate": int(current_sr or 22050)}

        final_subtitle = json.dumps(all_subtitles, ensure_ascii=False) if all_subtitles else ""

        filter_note_parts = []
        if include_roles_set:
            filter_note_parts.append(f"ä»…è§’è‰²: { 'ã€'.join(sorted(include_roles_set)) }")
        if exclude_roles_set:
            filter_note_parts.append(f"æ’é™¤è§’è‰²: { 'ã€'.join(sorted(exclude_roles_set)) }")
        if include_lines_set:
            filter_note_parts.append(f"æ®µè½: { 'ã€'.join(str(i) for i in sorted(include_lines_set)) }")

        header = [
            "=" * 90,
            f"ğŸ­ğŸµ åŠ¨æ€æƒ…ç»ªç”ŸæˆæŠ¥å‘Š | ç§å­: {seed} | è§’è‰²åŒ¹é…æ•°é‡: {len(stats['matched_roles'])} | å®é™…åŠ è½½è§’è‰²æ•°é‡: {len(stats['loaded_roles'])}",
            f"é‡‡æ ·é¢„è®¾: {preset_note} | temp={temp_val} top_p={top_p_val} top_k={top_k_val} beams={num_beams_val} rep={rep_val} len_pen={len_pen_val}",
            (f"ç­›é€‰: {' | '.join(filter_note_parts)}" if filter_note_parts else "ç­›é€‰: å…¨éƒ¨"),
            f"å¯¹è¯: {stats['dialog_count']} æ¡ | åœé¡¿: {stats['pause_count']} æ¡ | æ€»æ•°ï¼š{stats['dialog_count'] + stats['pause_count']} æ¡ | é»˜è®¤é¢„è®¾å‘½ä¸­: {stats['default_hit']} | è‡ªå®šä¹‰æƒ…ç»ªéŸ³è‰²å‘½ä¸­: {stats['custom_hit']} | åŸå§‹é‡‡æ ·éŸ³è‰²åº”ç”¨: {stats['raw_voice_hit']}",
            "-" * 90,
            "  Idx | çŠ¶æ€ | è§’è‰²       | æƒ…ç»ªæ ‡ç­¾     | æƒ…ç»ªå‘é‡å€¼                                       |   æ—¶é•¿",
            "--- | ---- | ---------- | ------------ | ------------------------------------------------ | ------",
        ]
        summary_lines = header + detail_rows
        summary_lines.append("-" * 90)
        if stats["unrecognized"]:
            summary_lines.append(f"âš ï¸ æœªè¯†åˆ«æƒ…ç»ª: {', '.join(stats['unrecognized'])}")
        if stats["warnings"]:
            summary_lines.append("è­¦å‘Š:")
            for w in stats["warnings"]:
                summary_lines.append(f"- {w}")
        parse_info = "\n".join(summary_lines)

        if unload_model:
            indextts_model.unload_model()

        end_time = time.time()
        print(f"ğŸ­ğŸµ æ€»ç”Ÿæˆæ—¶é—´: {end_time - start_time:.2f}ç§’ | å¹³å‡æ¯æ®µ: {(end_time - start_time)/len(segments):.2f}ç§’")

        return (merged_audio, all_audios, seed, final_subtitle, parse_info)

# æ³¨å†ŒèŠ‚ç‚¹
NODE_CLASS_MAPPINGS = {
    "buding_IndexTTSDynamicEmotion": buding_IndexTTSDynamicEmotion,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "buding_IndexTTSDynamicEmotion": "ğŸ­ IndexTTSåŠ¨æ€æƒ…ç»ªç”Ÿæˆå™¨ (Buding-tools)",
}

# å¯¼å‡ºçš„ç±»å
__all__ = ['NODE_CLASS_MAPPINGS', 'NODE_DISPLAY_NAME_MAPPINGS']
