"""
buding_SmartAudioSegmenter - æ™ºèƒ½éŸ³é¢‘åˆ‡å‰²å™¨
å…¼é¡¾æœ€é«˜ç²¾åº¦ï¼ˆSRTï¼‰å’Œæœ€å¤§çµæ´»æ€§ï¼ˆçº¯æ–‡æœ¬ï¼‰
"""

import os
import re
import json
import math
from pathlib import Path
from typing import List, Dict, Tuple, Optional

# å°è¯•å¯¼å…¥éŸ³é¢‘å¤„ç†åº“
try:
    import torchaudio
    import torch
    AUDIO_AVAILABLE = True
except ImportError:
    AUDIO_AVAILABLE = False
    print("âš ï¸ torchaudio æœªå®‰è£…ï¼Œå°†ä½¿ç”¨åŸºç¡€åŠŸèƒ½")

# å°è¯•å¯¼å…¥ Whisper
try:
    import whisper
    WHISPER_AVAILABLE = True
    print("âœ… openai-whisper å¯ç”¨")
except ImportError:
    WHISPER_AVAILABLE = False
    print("âš ï¸ whisper æœªå®‰è£…ï¼Œçº¯æ–‡æœ¬æ¨¡å¼ä¸å¯ç”¨")

# ComfyUI è¿›åº¦æ¡
try:
    from comfy.utils import ProgressBar
    COMFYUI_PROGRESSBAR = True
except ImportError:
    class ProgressBar:
        def __init__(self, total, *args, **kwargs):
            self.total = total
        def update(self, value=1, *args, **kwargs):
            pass
    COMFYUI_PROGRESSBAR = False

# SRT æ ¼å¼è¯†åˆ«æ­£åˆ™
SRT_REGEX = re.compile(r'\d+\n\d{2}:\d{2}:\d{2},\d{3} --> \d{2}:\d{2}:\d{2},\d{3}')

# JSON æ ¼å¼è¯†åˆ«æ­£åˆ™
JSON_REGEX = re.compile(r'^\s*\[\s*\{.*\}\s*\]\s*$', re.MULTILINE | re.DOTALL)

def is_srt_format(text: str) -> bool:
    """æ£€æŸ¥è¾“å…¥æ–‡æœ¬æ˜¯å¦ä¸º SRT æ ¼å¼"""
    return bool(SRT_REGEX.search(text.strip()))

def is_json_format(text: str) -> bool:
    """æ£€æŸ¥è¾“å…¥æ–‡æœ¬æ˜¯å¦ä¸º JSON æ ¼å¼"""
    try:
        # å…ˆæ£€æŸ¥åŸºæœ¬æ ¼å¼
        if not JSON_REGEX.match(text.strip()):
            return False
        
        # å°è¯•è§£æ JSON
        data = json.loads(text)
        if not isinstance(data, list):
            return False
        
        # æ£€æŸ¥æ¯ä¸ªæ¡ç›®çš„å¿…éœ€å­—æ®µ
        for item in data:
            if not isinstance(item, dict):
                return False
            # æ£€æŸ¥æ˜¯å¦æœ‰æ—¶é—´å­—æ®µ
            if 'start' not in item or 'end' not in item:
                return False
            # æ£€æŸ¥æ˜¯å¦æœ‰æ–‡æœ¬å­—æ®µï¼ˆæ”¯æŒå¤šç§å‘½åï¼‰
            if not any(key in item for key in ['å­—å¹•', 'text', 'content', 'dialogue']):
                return False
        
        return True
    except (json.JSONDecodeError, TypeError):
        return False

def parse_json_segments(text: str) -> List[Dict]:
    """è§£æ JSON æ ¼å¼çš„å­—å¹•ç‰‡æ®µ"""
    try:
        data = json.loads(text)
        segments = []
        
        for i, item in enumerate(data):
            # æå–æ–‡æœ¬ï¼ˆæ”¯æŒå¤šç§å­—æ®µåï¼‰
            text_content = ""
            for key in ['å­—å¹•', 'text', 'content', 'dialogue']:
                if key in item:
                    text_content = str(item[key])
                    break
            
            # æå–æ—¶é—´ä¿¡æ¯
            start_sec = float(item['start'])
            end_sec = float(item['end'])
            duration_sec = float(item.get('duration_sec', end_sec - start_sec))
            
            # æå– ID
            segment_id = item.get('id', f"segment_{i+1}")
            
            if start_sec < end_sec and text_content:
                segments.append({
                    "index": i + 1,
                    "id": segment_id,
                    "text": text_content,
                    "start_sec": start_sec,
                    "end_sec": end_sec,
                    "duration_sec": duration_sec,
                    "source": "json_timestamp",
                    "metadata": {k: v for k, v in item.items() 
                               if k not in ['start', 'end', 'duration_sec', 'å­—å¹•', 'text', 'content', 'dialogue']}
                })
        
        return segments
    except Exception as e:
        raise ValueError(f"JSON è§£æå¤±è´¥: {str(e)}")

def save_audio_with_format(waveform, sample_rate, filepath: str, format: str):
    """æ ¹æ®æ ¼å¼ä¿å­˜éŸ³é¢‘æ–‡ä»¶"""
    try:
        if format.lower() == "wav":
            torchaudio.save(filepath, waveform, sample_rate)
        elif format.lower() == "mp3":
            # å…ˆä¿å­˜ä¸º WAVï¼Œç„¶åè½¬æ¢ä¸º MP3ï¼ˆéœ€è¦ ffmpegï¼‰
            temp_wav = filepath.replace(".mp3", "_temp.wav")
            torchaudio.save(temp_wav, waveform, sample_rate)
            
            # ä½¿ç”¨ ffmpeg è½¬æ¢ä¸º MP3
            import subprocess
            subprocess.run([
                "ffmpeg", "-y", "-i", temp_wav, 
                "-codec:a", "libmp3lame", "-qscale:a", "2", 
                filepath
            ], check=True, capture_output=True)
            
            # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_wav):
                os.remove(temp_wav)
                
        elif format.lower() == "flac":
            torchaudio.save(filepath, waveform, sample_rate, 
                          encoding="PCM_S16", bits_per_sample=16)
        else:
            # é»˜è®¤ä¿å­˜ä¸º WAV
            torchaudio.save(filepath, waveform, sample_rate)
            
    except Exception as e:
        print(f"âš ï¸ æ— æ³•ä¿å­˜ä¸º {format} æ ¼å¼ï¼Œä½¿ç”¨ WAV æ ¼å¼: {str(e)}")
        # é™çº§ä¸º WAV
        wav_path = filepath.replace(f".{format}", ".wav")
        torchaudio.save(wav_path, waveform, sample_rate)
        return wav_path
    
    return filepath

def get_supported_input_formats() -> List[str]:
    """è·å–æ”¯æŒçš„è¾“å…¥éŸ³é¢‘æ ¼å¼"""
    formats = ["wav", "mp3", "flac", "ogg", "m4a", "aac"]
    
    # æ£€æŸ¥æ˜¯å¦å®‰è£…äº†é¢å¤–çš„è§£ç å™¨
    try:
        import ffmpeg
        # å¦‚æœæœ‰ ffmpegï¼Œæ”¯æŒæ›´å¤šæ ¼å¼
        formats.extend(["wma", "ape", "dsd"])
    except ImportError:
        pass
    
    return formats

def read_srt_file(srt_file_path: str) -> str:
    """è¯»å– SRT æ–‡ä»¶å†…å®¹"""
    if not srt_file_path or srt_file_path.strip() == "":
        return ""
    
    try:
        # å¤„ç†ç›¸å¯¹è·¯å¾„
        if not os.path.isabs(srt_file_path):
            # å°è¯•ç›¸å¯¹äºå½“å‰å·¥ä½œç›®å½•
            if os.path.exists(srt_file_path):
                file_path = srt_file_path
            else:
                # å°è¯•ç›¸å¯¹äº ComfyUI output ç›®å½•
                comfyui_output = "output"
                potential_path = os.path.join(comfyui_output, srt_file_path)
                if os.path.exists(potential_path):
                    file_path = potential_path
                else:
                    raise FileNotFoundError(f"æ‰¾ä¸åˆ° SRT æ–‡ä»¶: {srt_file_path}")
        else:
            file_path = srt_file_path
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        print(f"âœ… æˆåŠŸè¯»å– SRT æ–‡ä»¶: {file_path}")
        return content
        
    except FileNotFoundError as e:
        print(f"âŒ SRT æ–‡ä»¶ä¸å­˜åœ¨: {str(e)}")
        return ""
    except UnicodeDecodeError:
        # å°è¯•å…¶ä»–ç¼–ç 
        try:
            with open(file_path, 'r', encoding='gbk') as f:
                content = f.read()
            print(f"âœ… æˆåŠŸè¯»å– SRT æ–‡ä»¶ (GBKç¼–ç ): {file_path}")
            return content
        except Exception as e:
            print(f"âŒ è¯»å– SRT æ–‡ä»¶å¤±è´¥ (ç¼–ç é”™è¯¯): {str(e)}")
            return ""
    except Exception as e:
        print(f"âŒ è¯»å– SRT æ–‡ä»¶å¤±è´¥: {str(e)}")
        return ""

def srt_time_to_seconds(time_str: str) -> float:
    """å°† SRT æ—¶é—´æ ¼å¼è½¬æ¢ä¸ºç§’æ•°"""
    try:
        # ç§»é™¤å¯èƒ½çš„ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
        time_str = time_str.strip().replace(' ', '')
        # åˆ†å‰²æ—¶é—´éƒ¨åˆ†
        parts = re.split(r'[: ,]', time_str)
        if len(parts) >= 4:
            h, m, s, ms = map(int, parts[:4])
            return h * 3600 + m * 60 + s + ms / 1000.0
    except:
        pass
    return 0.0

def get_available_whisper_models() -> List[str]:
    """è·å–å¯ç”¨çš„ Whisper æ¨¡å‹åˆ—è¡¨"""
    if not WHISPER_AVAILABLE:
        return ["none"]
    
    # ä½¿ç”¨æ ‡å‡† whisper æ¨¡å‹åˆ—è¡¨
    models = ["none", "tiny", "base", "small", "medium", "large", "large-v2", "large-v3"]
    
    return models

class buding_SmartAudioSegmenter:
    """
    æ™ºèƒ½éŸ³é¢‘åˆ‡å‰²å™¨ - æ”¯æŒ SRT + Whisper åŒé‡æ¨¡å¼
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        # åŠ¨æ€è·å–å¯ç”¨çš„ Whisper æ¨¡å‹
        available_models = get_available_whisper_models()
        # è·å–æ”¯æŒçš„è¾“å‡ºæ ¼å¼
        output_formats = ["wav", "mp3", "flac"]
        
        return {
            "required": {
                "audio": ("AUDIO", {}),
                "reference_input": ("STRING", {
                    "multiline": True,
                    "default": '''1
00:00:00,000 --> 00:00:02,000
ç¬¬ä¸€å¥è¯

2
00:00:02,000 --> 00:00:04,000
ç¬¬äºŒå¥è¯''',
                    "tooltip": "æ”¯æŒå¤šç§æ ¼å¼ï¼š\nâ€¢ SRT å­—å¹•æ ¼å¼\nâ€¢ JSON æ—¶é—´è½´æ ¼å¼\nâ€¢ çº¯æ–‡æœ¬è„šæœ¬\nâ€¢ å¯è¿æ¥ TextFileLoader èŠ‚ç‚¹è¾“å‡º\n\næ”¯æŒè‡ªåŠ¨æ ¼å¼è¯†åˆ«"
                }),
                "input_format": (["auto", "srt", "json", "text"], {
                    "default": "auto",
                    "tooltip": "auto: è‡ªåŠ¨è¯†åˆ«æ ¼å¼\nsrt: å¼ºåˆ¶æŒ‰ SRT è§£æ\njson: å¼ºåˆ¶æŒ‰ JSON è§£æ\ntext: å¼ºåˆ¶æŒ‰çº¯æ–‡æœ¬å¤„ç†"
                }),
                "whisper_model": (available_models, {
                    "default": available_models[0] if available_models else "none",
                    "tooltip": "Whisper æ¨¡å‹é€‰æ‹©\nçº¯æ–‡æœ¬æ¨¡å¼æ—¶ä½¿ç”¨"
                }),
                "language": (["auto", "zh", "en", "ja", "ko"], {
                    "default": "auto",
                    "tooltip": "éŸ³é¢‘è¯­è¨€è¯†åˆ«\nauto: è‡ªåŠ¨æ£€æµ‹"
                }),
                "output_dir": ("STRING", {
                    "default": "output/audio_segments",
                    "tooltip": "åˆ‡å‰²éŸ³é¢‘çš„ä¿å­˜ç›®å½•"
                }),
            },
            "optional": {
                "format": (output_formats, {
                    "default": "wav",
                    "tooltip": "è¾“å‡ºéŸ³é¢‘æ ¼å¼\nâ€¢ wav: æ— æŸå…¼å®¹æ€§æœ€ä½³\nâ€¢ mp3: å‹ç¼©æ–‡ä»¶å°\nâ€¢ flac: æ— æŸé«˜è´¨é‡"
                }),
                "overwrite": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "æ˜¯å¦è¦†ç›–å·²å­˜åœ¨çš„æ–‡ä»¶\nâ€¢ False: è·³è¿‡å·²å­˜åœ¨æ–‡ä»¶\nâ€¢ True: å¼ºåˆ¶é‡æ–°ç”Ÿæˆæ‰€æœ‰æ–‡ä»¶"
                }),
                "time_tolerance": ("FLOAT", {
                    "default": 0.5,
                    "min": 0.1,
                    "max": 2.0,
                    "step": 0.1,
                    "tooltip": "æ—¶é—´å®¹å¿åº¦ï¼ˆç§’ï¼‰\nç”¨äº Whisper éªŒè¯"
                }),
            }
        }
    
    RETURN_TYPES = ("STRING", "STRING", "STRING")
    RETURN_NAMES = ("SEGMENTS_JSON", "OUTPUT_DIR_PATH", "PROCESS_REPORT")
    FUNCTION = "segment_audio"
    CATEGORY = "Buding-time/Audio"
    DESCRIPTION = "æ™ºèƒ½éŸ³é¢‘åˆ‡å‰²å™¨ - æ”¯æŒ SRT æ—¶é—´æˆ³å’Œ Whisper å¼ºåˆ¶å¯¹é½"
    
    def segment_audio(self, audio, reference_input: str, input_format: str, 
                      whisper_model: str, language: str, output_dir: str,
                      format: str = "wav", overwrite: bool = False, 
                      time_tolerance: float = 0.5) -> Tuple[str, str, str]:
        
        pbar = ProgressBar(100)
        
        try:
            # Step 1: æ ¼å¼è¯†åˆ«
            if COMFYUI_PROGRESSBAR:
                print("ğŸ” æ­£åœ¨åˆ†æè¾“å…¥æ ¼å¼...")
            pbar.update(5)

            # æ™ºèƒ½æ ¼å¼æ£€æµ‹
            if input_format == "auto":
                if is_srt_format(reference_input):
                    format_detected = "srt"
                elif is_json_format(reference_input):
                    format_detected = "json"
                else:
                    format_detected = "text"
            else:
                format_detected = input_format
            
            if COMFYUI_PROGRESSBAR:
                print(f"ğŸ” æ£€æµ‹åˆ° {format_detected.upper()} æ ¼å¼...")
            pbar.update(10)
            
            # æ ¹æ®æ ¼å¼å¤„ç†
            if format_detected == "srt":
                segments_data = self._process_srt_mode(reference_input, audio, pbar)
            elif format_detected == "json":
                segments_data = self._process_json_mode(reference_input, audio, pbar)
            else:  # text
                segments_data = self._process_text_mode(reference_input, audio, whisper_model, language, pbar)
            
            # Step 2: éŸ³é¢‘åˆ‡å‰²
            if COMFYUI_PROGRESSBAR:
                print("ğŸ” å¼€å§‹éŸ³é¢‘åˆ‡å‰²...")
            pbar.update(60)
            output_path = Path(output_dir)
            output_path.mkdir(parents=True, exist_ok=True)
            
            # æ£€æŸ¥è¦†ç›–é€‰é¡¹
            if not overwrite:
                # æ£€æŸ¥å·²å­˜åœ¨çš„æ–‡ä»¶
                existing_files = []
                for segment in segments_data:
                    filename = f"segment_{segment['index']:03d}.{format}"
                    filepath = output_path / filename
                    if filepath.exists():
                        existing_files.append(filename)
                        segment["skipped"] = True
                        segment["audio_path"] = str(filepath)
                        segment["audio_filename"] = filename
                
                if existing_files:
                    print(f"âš ï¸ å‘ç° {len(existing_files)} ä¸ªå·²å­˜åœ¨æ–‡ä»¶ï¼Œè·³è¿‡å¤„ç†")
                    print(f"   æ–‡ä»¶: {', '.join(existing_files[:5])}{'...' if len(existing_files) > 5 else ''}")
            
            # åªå¤„ç†ä¸å­˜åœ¨çš„æ–‡ä»¶
            segments_to_process = [seg for seg in segments_data if not seg.get("skipped", False)]
            
            if segments_to_process:
                # åˆ‡å‰²éŸ³é¢‘
                segments_data = self._cut_audio_segments(audio, segments_data, output_path, format, pbar)
            else:
                print("âœ… æ‰€æœ‰æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡éŸ³é¢‘åˆ‡å‰²")
                pbar.update(90)
            
            # Step 3: ç”ŸæˆæŠ¥å‘Š
            if COMFYUI_PROGRESSBAR:
                print("ğŸ” ç”Ÿæˆå¤„ç†æŠ¥å‘Š...")
            pbar.update(90)
            segments_json = json.dumps(segments_data, ensure_ascii=False, indent=2)
            process_report = self._generate_report(segments_data, format_detected, whisper_model)
            
            pbar.update(100)
            if COMFYUI_PROGRESSBAR:
                print("ğŸ‰ å¤„ç†å®Œæˆï¼")
            
            return (segments_json, str(output_path), process_report)
            
        except Exception as e:
            error_msg = f"âŒ éŸ³é¢‘åˆ‡å‰²å¤±è´¥: {str(e)}"
            print(error_msg)
            return (json.dumps([], ensure_ascii=False), "", error_msg)
    
    def _process_json_mode(self, json_text: str, audio, pbar: ProgressBar) -> List[Dict]:
        """JSON æ¨¡å¼å¤„ç† - ç›´æ¥ä½¿ç”¨æ—¶é—´æˆ³"""
        if COMFYUI_PROGRESSBAR:
            print("ğŸ” è§£æ JSON æ—¶é—´è½´...")
        pbar.update(15)
        
        try:
            segments = parse_json_segments(json_text)
            
            if COMFYUI_PROGRESSBAR:
                print(f"ğŸ” JSON è§£æå®Œæˆï¼Œå…± {len(segments)} ä¸ªç‰‡æ®µ")
            pbar.update(25)
            return segments
            
        except Exception as e:
            raise ValueError(f"JSON æ ¼å¼è§£æå¤±è´¥: {str(e)}")
    
    def _process_srt_mode(self, srt_text: str, audio, pbar: ProgressBar) -> List[Dict]:
        """SRT æ¨¡å¼å¤„ç† - ç›´æ¥ä½¿ç”¨æ—¶é—´æˆ³"""
        if COMFYUI_PROGRESSBAR:
            print("ğŸ” è§£æ SRT æ—¶é—´è½´...")
        pbar.update(15)
        
        segments = []
        blocks = re.split(r'\n\s*\n', srt_text.strip())
        
        for block in blocks:
            lines = block.strip().split('\n')
            if len(lines) < 3:
                continue
            
            # è§£ææ—¶é—´è½´
            time_line = lines[1]
            if " --> " not in time_line:
                continue
            
            start_str, end_str = time_line.split(" --> ")
            start_sec = srt_time_to_seconds(start_str.strip())
            end_sec = srt_time_to_seconds(end_str.strip())
            
            # æå–æ–‡æœ¬
            text = "\n".join(lines[2:]).strip()
            
            if start_sec < end_sec and text:
                segments.append({
                    "index": len(segments) + 1,
                    "text": text,
                    "start_sec": start_sec,
                    "end_sec": end_sec,
                    "duration_sec": end_sec - start_sec,
                    "source": "srt_timestamp"
                })
        
        if COMFYUI_PROGRESSBAR:
            print(f"ğŸ” SRT è§£æå®Œæˆï¼Œå…± {len(segments)} ä¸ªç‰‡æ®µ")
        pbar.update(25)
        return segments
    
    def _process_text_mode(self, text: str, audio, whisper_model: str, language: str, pbar: ProgressBar) -> List[Dict]:
        """çº¯æ–‡æœ¬æ¨¡å¼ - ä½¿ç”¨ Whisper å¼ºåˆ¶å¯¹é½"""
        if not WHISPER_AVAILABLE:
            raise ImportError("Whisper æœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨çº¯æ–‡æœ¬æ¨¡å¼")
        
        if COMFYUI_PROGRESSBAR:
            print("ğŸ” åŠ è½½ Whisper æ¨¡å‹...")
        pbar.update(15)
        
        try:
            # ä½¿ç”¨æ ‡å‡† whisperï¼Œå‚è€ƒ comfyui-edgetts çš„å®ç°
            model = whisper.load_model(whisper_model)
            
            if COMFYUI_PROGRESSBAR:
                print("ğŸ” å¼€å§‹è¯­éŸ³è¯†åˆ«...")
            pbar.update(20)
            
            # è·å–éŸ³é¢‘æ•°æ®å¹¶ä¿å­˜ä¸´æ—¶æ–‡ä»¶
            if AUDIO_AVAILABLE:
                waveform = audio["waveform"]
                sample_rate = audio["sample_rate"]
                
                # è½¬æ¢ä¸ºå•å£°é“
                if waveform.dim() == 3:
                    waveform = waveform.squeeze(0)
                if waveform.shape[0] > 1:
                    waveform = waveform.mean(dim=0, keepdim=True)
                
                # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
                temp_file = f"temp_whisper_input_{os.getpid()}.wav"
                torchaudio.save(temp_file, waveform, sample_rate)
                
                # æ ‡å‡† whisper è¯†åˆ«
                result = model.transcribe(
                    temp_file, 
                    language=None if language == "auto" else language,
                    word_timestamps=True,
                    verbose=False
                )
                
                result_segments = result["segments"]
                
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if os.path.exists(temp_file):
                    os.remove(temp_file)
            else:
                raise ImportError("torchaudio æœªå®‰è£…ï¼Œæ— æ³•å¤„ç†éŸ³é¢‘")
            
            if COMFYUI_PROGRESSBAR:
                print("ğŸ” Whisper è¯†åˆ«å®Œæˆï¼Œå¼€å§‹æ–‡æœ¬å¯¹é½...")
            pbar.update(40)
            
            # æ–‡æœ¬å¯¹é½é€»è¾‘
            ref_texts = [line.strip() for line in text.split('\n') if line.strip()]
            segments = []
            
            current_word_index = 0
            for i, ref_text in enumerate(ref_texts):
                if current_word_index >= len(result_segments):
                    break
                
                # æ‰¾åˆ°æœ€åŒ¹é…çš„ Whisper ç‰‡æ®µ
                best_segment = result_segments[current_word_index]
                
                segments.append({
                    "index": i + 1,
                    "text": ref_text,
                    "start_sec": best_segment["start"],
                    "end_sec": best_segment["end"],
                    "duration_sec": best_segment["end"] - best_segment["start"],
                    "source": "whisper_alignment",
                    "confidence": best_segment.get("avg_logprob", 0)
                })
                
                current_word_index += 1
            
            if COMFYUI_PROGRESSBAR:
                print(f"ğŸ” Whisper å¯¹é½å®Œæˆï¼Œå…± {len(segments)} ä¸ªç‰‡æ®µ")
            pbar.update(25)
            return segments
            
        except Exception as e:
            raise Exception(f"Whisper å¤„ç†å¤±è´¥: {str(e)}")
    
    def _cut_audio_segments(self, audio, segments_data: List[Dict], output_path: Path, 
                           format: str, pbar: ProgressBar) -> List[Dict]:
        """åˆ‡å‰²éŸ³é¢‘ç‰‡æ®µ"""
        if not AUDIO_AVAILABLE:
            print("âš ï¸ éŸ³é¢‘å¤„ç†åº“ä¸å¯ç”¨ï¼Œè·³è¿‡æ–‡ä»¶ä¿å­˜")
            return segments_data
        
        waveform = audio["waveform"]
        sample_rate = audio["sample_rate"]
        total_samples = waveform.shape[-1]
        
        progress_per_segment = 30 / len(segments_data) if segments_data else 0
        
        for i, segment in enumerate(segments_data):
            start_sample = int(segment["start_sec"] * sample_rate)
            end_sample = int(segment["end_sec"] * sample_rate)
            
            # è¾¹ç•Œæ£€æŸ¥
            start_sample = max(0, min(start_sample, total_samples))
            end_sample = max(start_sample, min(end_sample, total_samples))
            
            # æå–ç‰‡æ®µ
            segment_waveform = waveform[..., start_sample:end_sample]
            
            # ä¿å­˜æ–‡ä»¶ï¼ˆä½¿ç”¨æ–°çš„æ ¼å¼ä¿å­˜å‡½æ•°ï¼‰
            filename = f"segment_{segment['index']:03d}.{format}"
            filepath = output_path / filename
            
            try:
                saved_path = save_audio_with_format(
                    segment_waveform.squeeze(0), 
                    sample_rate, 
                    str(filepath), 
                    format
                )
                # æ›´æ–°å®é™…ä¿å­˜çš„è·¯å¾„ï¼ˆå¯èƒ½å› ä¸ºæ ¼å¼è½¬æ¢è€Œæ”¹å˜ï¼‰
                segment["audio_path"] = saved_path
                segment["audio_filename"] = os.path.basename(saved_path)
            except Exception as e:
                print(f"âš ï¸ ä¿å­˜ç‰‡æ®µ {i+1} å¤±è´¥: {str(e)}")
                # ä¿å­˜ä¸ºå¤‡ç”¨ WAV
                backup_filename = f"segment_{segment['index']:03d}_backup.wav"
                backup_path = output_path / backup_filename
                torchaudio.save(str(backup_path), segment_waveform.squeeze(0), sample_rate)
                segment["audio_path"] = str(backup_path)
                segment["audio_filename"] = backup_filename
            
            if COMFYUI_PROGRESSBAR:
                print(f"ğŸ” åˆ‡å‰²ç‰‡æ®µ {i+1}/{len(segments_data)} å®Œæˆ")
            pbar.update(60 + i * progress_per_segment)
        
        return segments_data
    
    def _generate_report(self, segments_data: List[Dict], format_type: str, whisper_model: str) -> str:
        """ç”Ÿæˆå¤„ç†æŠ¥å‘Š"""
        lines = []
        lines.append("=" * 80)
        lines.append("ğŸµ Buding Smart Audio Segmenter - å¤„ç†æŠ¥å‘Š")
        lines.append("=" * 80)
        
        # åŸºæœ¬ä¿¡æ¯
        lines.append(f"ğŸ“‹ è¾“å…¥æ ¼å¼: {format_type.upper()}")
        if format_type == "text":
            lines.append(f"ğŸ¤– Whisper æ¨¡å‹: {whisper_model}")
        lines.append(f"ğŸ“Š å¤„ç†ç‰‡æ®µæ•°: {len(segments_data)}")
        lines.append("")
        
        # æ—¶é—´è½´ç»Ÿè®¡
        if segments_data:
            total_duration = sum(seg["duration_sec"] for seg in segments_data)
            lines.append("â±ï¸ æ—¶é—´è½´ç»Ÿè®¡:")
            lines.append(f"   æ€»æ—¶é•¿: {total_duration:.2f} ç§’")
            lines.append(f"   å¹³å‡ç‰‡æ®µæ—¶é•¿: {total_duration/len(segments_data):.2f} ç§’")
            lines.append("")
            
            # ç‰‡æ®µè¯¦æƒ…
            lines.append("ğŸ“ ç‰‡æ®µè¯¦æƒ…:")
            lines.append("-" * 80)
            for seg in segments_data:
                start_time = self._seconds_to_time_str(seg["start_sec"])
                end_time = self._seconds_to_time_str(seg["end_sec"])
                lines.append(f"ç‰‡æ®µ {seg['index']:3d}: {start_time} --> {end_time} ({seg['duration_sec']:.2f}s)")
                lines.append(f"        {seg['text'][:50]}{'...' if len(seg['text']) > 50 else ''}")
                if seg.get("id"):
                    lines.append(f"        ID: {seg['id']}")
                if seg.get("metadata"):
                    lines.append(f"        å…ƒæ•°æ®: {seg['metadata']}")
                lines.append("")
        
        # ASCII æ—¶é—´è½´
        lines.append("ğŸ“Š æ—¶é—´è½´å¯è§†åŒ–:")
        lines.append("-" * 80)
        timeline = self._generate_timeline(segments_data)
        lines.append(timeline)
        
        # æ ¼å¼è¯´æ˜
        lines.append("")
        lines.append("ğŸ“– æ ¼å¼è¯´æ˜:")
        if format_type == "srt":
            lines.append("   â€¢ SRT å­—å¹•æ ¼å¼ - ç²¾ç¡®æ—¶é—´æˆ³åˆ‡å‰²")
        elif format_type == "json":
            lines.append("   â€¢ JSON æ—¶é—´è½´æ ¼å¼ - çµæ´»å…ƒæ•°æ®æ”¯æŒ")
        elif format_type == "text":
            lines.append("   â€¢ çº¯æ–‡æœ¬ + Whisper - è‡ªåŠ¨è¯­éŸ³è¯†åˆ«å¯¹é½")
        
        lines.append("=" * 80)
        
        return "\n".join(lines)
    
    def _generate_timeline(self, segments_data: List[Dict]) -> str:
        """ç”Ÿæˆ ASCII æ—¶é—´è½´å¯è§†åŒ–"""
        if not segments_data:
            return "æ— ç‰‡æ®µæ•°æ®"
        
        # è®¡ç®—æ€»æ—¶é•¿
        total_duration = max(seg["end_sec"] for seg in segments_data)
        timeline_width = 60  # ASCII æ—¶é—´è½´å®½åº¦
        
        lines = []
        lines.append("æ—¶é—´è½´ (ç§’):")
        lines.append("â”Œ" + "â”€" * timeline_width + "â”")
        
        # æ—¶é—´åˆ»åº¦
        for i in range(0, int(total_duration) + 1, max(1, int(total_duration) // 10)):
            pos = int(i / total_duration * timeline_width) if total_duration > 0 else 0
            lines.append("â”‚" + " " * pos + str(i) + " " * (timeline_width - pos - len(str(i))) + "â”‚")
        
        lines.append("â””" + "â”€" * timeline_width + "â”˜")
        lines.append("")
        
        # ç‰‡æ®µæ ‡è®°
        lines.append("ç‰‡æ®µåˆ†å¸ƒ:")
        for i, seg in enumerate(segments_data):
            start_pos = int(seg["start_sec"] / total_duration * timeline_width) if total_duration > 0 else 0
            end_pos = int(seg["end_sec"] / total_duration * timeline_width) if total_duration > 0 else 0
            
            timeline_line = "â”‚" + " " * start_pos
            timeline_line += "â–ˆ" * (end_pos - start_pos)
            timeline_line += " " * (timeline_width - end_pos)
            timeline_line += "â”‚"
            
            lines.append(f"ç‰‡æ®µ{i+1:2d}: {timeline_line}")
            lines.append(f"        {seg['start_sec']:.1f}s - {seg['end_sec']:.1f}s ({seg['duration_sec']:.1f}s)")
        
        return "\n".join(lines)
    
    def _seconds_to_time_str(self, seconds: float) -> str:
        """å°†ç§’æ•°è½¬æ¢ä¸ºæ—¶é—´å­—ç¬¦ä¸²"""
        h = int(seconds // 3600)
        m = int((seconds % 3600) // 60)
        s = int(seconds % 60)
        ms = int((seconds % 1) * 1000)
        return f"{h:02d}:{m:02d}:{s:02d},{ms:03d}"

# èŠ‚ç‚¹æ˜ å°„
NODE_CLASS_MAPPINGS = {
    "buding_SmartAudioSegmenter": buding_SmartAudioSegmenter,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "buding_SmartAudioSegmenter": "ğŸµ Buding - æ™ºèƒ½éŸ³é¢‘åˆ‡å‰²å™¨ï¼ˆSRT+Whisperï¼‰",
}

__all__ = ["NODE_CLASS_MAPPINGS", "NODE_DISPLAY_NAME_MAPPINGS"]
